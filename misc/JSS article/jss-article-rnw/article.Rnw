\documentclass[nojss]{jss} % article

%% -- LaTeX packages and custom commands ---------------------------------------

%% recommended packages
\usepackage{orcidlink,thumbpdf,lmodern}

%% another package (only for this demo article)
\usepackage{framed}

%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}

%% For Sweave-based articles about R packages:
%% need no \usepackage{Sweave}
%% need no \usepackage{Sweave.sty}
\SweaveOpts{engine=R, eps=FALSE, keep.source = TRUE, cache=TRUE}
% \renewenvironment{knitrout}{\setlength{\topsep}{0mm}}{}

<<preliminaries, echo=FALSE, message=FALSE, warning=FALSE, results=hide>>=
# knitr::opts_chunk$set(cache = FALSE, size = "small")
options(prompt = "R> ", continue = "+  ", width = 77, digits = 4, useFancyQuotes = FALSE, warn = 1)
# Loading libraries and installing if unavailable
if(!requireNamespace("fastverse", quietly = TRUE)) install.packages("fastverse")
library(fastverse) # loads data.table, collapse, magrittr and kit (not used)
fastverse_extend(microbenchmark, Rfast, fixest, install = TRUE) # loads and installs if unavailable
# Package versions used in the article:
# fastverse 0.3.2, collapse 2.0.10, data.table 1.15.0, magrittr 2.0.3,
# microbenchmark 1.4.10, Rfast 2.1.0, and fixest 0.11.3
@


%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation (and optionally ORCID link)
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{Sebastian Krantz~\orcidlink{0000-0001-6212-5229}\\Kiel Institute for the World Economy}
\Plainauthor{Sebastian Krantz}

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{\proglang{collapse}: Advanced and Fast Statistical Computing and Data Transformation in \proglang{R}}
\Plaintitle{collapse: Advanced and Fast Statistical Computing and Data Transformation in R}
\Shorttitle{\proglang{collapse}: Advanced and Fast Data Transformation in \proglang{R}}

%% - \Abstract{} almost as usual
\Abstract{
\pkg{collapse} is a large \proglang{C/C++}-based infrastructure package facilitating complex statistical computing, data transformation, and exploration tasks in \proglang{R} - at outstanding levels of performance and memory efficiency. It also implements a class-agnostic approach to \proglang{R} programming, supporting vector, matrix and data frame-like objects and their popular variants (e.g., \class{factor}, \class{ts}, \class{xts}, \class{tibble}, \class{data.table}, \class{sf}), enabling its seamless integration with large parts of the \proglang{R} ecosystem. This article introduces the package's key components and design principles in a structured way, supported by a rich set of examples. A small benchmark demonstrates its computational performance.
%  This short article illustrates how to write a manuscript for the
%  \emph{Journal of Statistical Software} (JSS) using its {\LaTeX} style files.
%  Generally, we ask to follow JSS's style guide and FAQs precisely. Also,
%  it is recommended to keep the {\LaTeX} code as simple as possible,
%  i.e., avoid inclusion of packages/commands that are not necessary.
%  For outlining the typical structure of a JSS article some brief text snippets
%  are employed that have been inspired by \cite{Zeileis+Kleiber+Jackman:2008},
%  discussing count data regression in \proglang{R}. Editorial comments and
%  instructions are marked by vertical bars.
}

%% - \Keywords{} with LaTeX markup, at least one required
%% - \Plainkeywords{} without LaTeX markup (if necessary)
%% - Should be comma-separated and in sentence case.
\Keywords{statistical computing, vectorization, data manipulation and transformation, summary statistics, class-agnostic programming, \proglang{R}}
\Plainkeywords{statistical computing, vectorization, data transformation and manipulation, summary statistics, class-agnostic programming, R}

%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and}\\).
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).
\Address{
  Sebastian Krantz\\
  Kiel Institute for the World Economy\\
  Haus Welt-Club\\
  D\"usternbrooker Weg 148\\
  24105 Kiel, Germany\\
  E-mail: \email{sebastian.krantz@ifw-kiel.de}
}

\begin{document}

%% -- Introduction -------------------------------------------------------------

%% - In principle "as usual".
%% - But should typically have some discussion of both _software_ and _methods_.
%% - Use \proglang{}, \pkg{}, and \code{} markup throughout the manuscript.
%% - If such markup is in (sub)section titles, a plain text version has to be
%%   added as well.
%% - All software mentioned should be properly \cite-d.
%% - All abbreviations should be introduced.
%% - Unless the expansions of abbreviations are proper names (like "Journal
%%   of Statistical Software" above) they should be in sentence case (like
%%   "generalized linear models" below).

\section[Introduction]{Introduction} \label{sec:intro}
%
\href{https://sebkrantz.github.io/collapse/}{\pkg{collapse}}\footnote{Website: https://sebkrantz.github.io/collapse/} is a large \proglang{C/C++}-based \proglang{R} package that provides an integrated suite of statistical and data manipulation functions. Most of these statistical functions are vectorized along multiple dimensions (notably along groups and columns) and perform high-cardinality operations\footnote{With many columns and/or groups relative to data size.} very efficiently. It also offers vectorizations for advanced operations such as weighted statistics (including mode and quantiles), functions and classes for fully indexed (time-aware) computations on time series and panel data, recursive (list-processing) tools to deal with nested data and advanced descriptive statistical tools. This functionality is supported by efficient algorithms for intensive operations like grouping, unique values, matching, ordering, etc., tailored to \proglang{R}'s data structures, and powerful data manipulation functions. The package also supplies many features for memory efficient \proglang{R} programming, such as data transformation and math by reference, and aversion of logical vectors. \pkg{collapse} is class-agnostic, i.e., it provides most statistical operations for atomic vectors, matrices, and data frames/lists, and seamlessly supports key variants of these objects used in the \proglang{R} ecosystem (e.g., \class{tibble}, \class{data.table}, \class{sf}, \class{xts}, \class{pdata.frame}). It is globally and interactively configurable, which includes setting different defaults for key function arguments (such as \code{na.rm} arguments to statistical functions, default \code{TRUE}), and modifying the package namespace itself.\footnote{\pkg{collapse}'s namespace is fully compatible with base \proglang{R} and the \pkg{tidyverse}, but can be interactively modified to overwrite key functions like \code{unique}, \code{match}, \code{\%in\%}, \code{table}, \code{subset}, \code{mutate}, \code{summarise} etc. with much faster \pkg{collapse} equivalents. See Section~\ref{sec:glob_opt}.} \newline

What is the purpose of combining all of this in a package? The short answer is to make computations in \proglang{R} as flexible and powerful as possible. The more elaborate answer is to (1) facilitate complex data transformation, exploration, and computing tasks in \proglang{R}; (2) increase performance and parsimony by avoiding \proglang{R}-level repetition;\footnote{Such as applying \proglang{R} functions across columns or split-apply-combine computing to apply functions across groups or other divisions of data.} (3) increase the memory efficiency and flexibility of \proglang{R} programs;\footnote{E.g., by avoiding object conversions and the need for certain classes to do certain things, such as converting to data frame or \class{data.table} to do something "by groups" and then convert back to matrix to continue with linear algebra, and in general to reduce the need for metaprogramming.} and (4) to create a new foundation package for statistics and data manipulation in \proglang{R} that implements successful ideas developed in the \proglang{R} ecosystem and other programming environments such as \proglang{Python} or \proglang{STATA} \citep{STATA}, including some new ideas, in a stable, high performance, and broadly compatible manor.\footnote{Examples of such ideas are \pkg{tidyverse} syntax, vectorized aggregations (\pkg{data.table}), data transformation by reference (\proglang{Python}, \pkg{pandas}), vectorized and verbose joins (\pkg{polars}, \proglang{STATA}), indexed time series and panel data (\pkg{xts}, \pkg{plm}), summary statistics for panel data (\proglang{STATA}), reshaping labelled data (myself) etc...} \newline

R already has a large and tested data manipulation and statistical computing ecosystem. Notably, the \pkg{tidyverse} \citep{rtidyverse} provides a consistent toolkit for data manipulation in R, centered around the \class{tibble} \citep{rtibble} object and tidy data principles \citep{rtidydata}. \pkg{data.table} \citep{rdatatable} provides an enhanced high-performance data frame with parsimonious data manipulation syntax. \pkg{sf} \citep{rsf} provides a data frame for spatial data and supporting functionality. \pkg{tsibble} \citep{rtsibble} and \pkg{xts} \citep{rxts} provide classes and operations for time series data, the former via an enhanced \class{tibble}, the latter through an efficient matrix-based class. Econometric packages like \pkg{plm} \citep{rplm} and \pkg{fixest} \citep{rfixest} also provide solutions to deal with panel data and irregularity in the time dimension. Packages like \pkg{matrixStats} \citep{rmatrixstats} and \pkg{Rfast} \citep{rfast} offer fast statistical calculations along the rows and columns of matrices and faster basic statistical procedures. \pkg{DescTools} \citep{rdesctools} provides a wide variety of descriptive statistics, including weighted versions. \pkg{survey} \citep{rsurvey} allows statistical computations on complex survey data. \pkg{labelled} \citep{rlabelled} provides tools to deal with labelled data. Packages like \pkg{tidyr} \citep{rtidyr}, \pkg{purrr} \citep{rpurrr} and \pkg{rrapply} \citep{rrapply} provide some functions to deal with nested data and messy structures. \newline

\pkg{collapse} relates to and integrates key elements from these projects. It offers \pkg{tidyverse}-like data manipulation at the speed and stability of \pkg{data.table} for any data frame-like object. It can turn any vector/matrix/data frame into a time-aware indexed series or frame and perform operations such as lagging, differencing, scaling or centering, encompassing and enhancing core manipulation functionality of \pkg{plm}, \pkg{fixest}, and \pkg{xts}. It also performs fast (grouped, weighted) statistical computations along the columns of matrix-like objects, complementing and enhancing \pkg{matrixStats} and \pkg{Rfast}. Its low-level vectorizations and workhorse algorithms are accessible at the \proglang{R} and \proglang{C}-levels, unlike \pkg{data.table}, where most vectorizations and algorithms are internal. It also supports variable labels and intelligently preserves attributes of all objects, complementing \pkg{labelled}. It provides general (recursive) tools to deal with nested data, enhancing \pkg{tidyr}, \pkg{purrr}, and \pkg{rrapply}. Finally, it provides a small but consistent and powerful set of descriptive statistical tools, yielding sufficient detail for most data exploration purposes, requiring users to invoke packages like \pkg{DescTools} or \pkg{survey} only for specific statistics. In summary, \pkg{collapse} is a foundation package for statistical computing and data manipulation in \proglang{R} that enhances and integrates seamlessly with the \proglang{R} ecosystem while being outstandingly computationally efficient. A significant benefit is that, rather than piecing together a fragmented ecosystem oriented at different classes and tasks, many core computational tasks can be done with \pkg{collapse}, and easily extended by more specialized packages. This tends to result in \proglang{R} scripts that are shorter, more efficient, and more lightweight in dependencies. \newline
% TODO: Need examples of this

Other programming environments such as \proglang{Python} and \proglang{Julia} now also offer computationally very powerful libraries for tabular data such as \pkg{DataFrames.jl} \citep{jldataframes}, \pkg{Polars} \citep{pypolars}, and \pkg{Pandas} \citep{mckinney2010pandas, pypandas}, and supporting numerical libraries such as \pkg{Numpy} \citep{pynumpy}, or \pkg{StatsBase.jl} \citep{jlstatsbase}. % \pkg{NaNStatistics.jl} \citep{jlnanstatistics}
In comparison with these, \pkg{collapse} offers a class-agnostic approach bridging the divide between data frames and atomic structures, has more advanced statistical capabilities,\footnote{Such as weighted statistics, including various quantile and mode estimators, support for fully time-aware computations on irregular series/panels, higher order centering, advanced (grouped, weighted, panel-decomposed) descriptive statistics etc., all supporting missing values.} supports recursive operations, variable labels, verbosity for critical operations such as joins, and is extensively globally configurable. In short, it is very utile for complex statistical workflows, rich datasets (e.g., surveys), and for integrating with different parts of the \proglang{R} ecosystem. On the other hand, \pkg{collapse}, for the most part, does not offer a sub-column-level parallel architecture and is thus not highly competitive with top frameworks, including \pkg{data.table}, on aggregating billion-row datasets with few columns.\footnote{As can be seen in the \href{https://duckdblabs.github.io/db-benchmark/}{DuckDB Benchmarks}: \pkg{collapse} is highly competitive on the 10-100 million observations datasets, but deteriorates in performance at larger data sizes (except for joins where it remains competitive). There may be performance improvements for "long data" in the future, but, at present, the treatment of columns as fundamental units of computation is a tradeoff for the highly flexible class-agnostic architecture.} Its vectorization capabilities are also limited to the statistical functions it provides and not, like \pkg{DataFrames.jl}, to any \proglang{Julia} function. However, as demonstrated in Section~\ref{ssec:vfat}, vectorized statistical functions can be combined to calculate more complex statistics in a vectorized way. \newline

The package has a built-in structured \href{https://sebkrantz.github.io/collapse/reference/collapse-documentation.html}{documentation} facilitating its use. This documentation includes a central \href{https://sebkrantz.github.io/collapse/reference/collapse-documentation.html}{overview page} linking to all other documentation pages and supplementary topic pages which briefly describe related functionality. The names of these extra pages are collected in a global macro \code{.COLLAPSE\_TOPICS} and can be called directly with \code{help()}:
%
<<collapse_topics>>=
.COLLAPSE_TOPICS
help("collapse-documentation")
@
%
\pkg{collapse} is too large and complex to fully present it in a single article, or even to present selected topics in depth. The following sections therefore briefly introduce its key components: (\ref{sec:fast_stat_fun}) the \emph{Fast Statistical Functions} and their (\ref{sec:integration}) integration with data manipulation functions; (\ref{sec:ts_ps}) architecture for time series and panel data; (\ref{sec:join_pivot}) joins and reshaping; (\ref{sec:list_proc}) list processing functions; (\ref{sec:summ_stat}) descriptive tools; and (\ref{sec:glob_opt}) global options. Section~\ref{sec:bench} provides a small benchmark, Section~\ref{sec:conclusion} concludes. For deeper engagement with \pkg{collapse}, a short vignette summarizing available \href{https://sebkrantz.github.io/collapse/articles/collapse_documentation.html}{documentation and resources} is an excellent starting point.
%
\section{Fast statistical functions} \label{sec:fast_stat_fun}
%
The \href{https://sebkrantz.github.io/collapse/reference/fast-statistical-functions.html}{\emph{Fast Statistical Functions}}, comprising \fct{fsum}, \fct{fprod}, \fct{fmean}, \fct{fmedian}, \fct{fmode}, \fct{fvar}, \fct{fsd}, \fct{fmin}, \fct{fmax}, \fct{fnth}, \fct{ffirst}, \fct{flast}, \fct{fnobs} and \fct{fndistinct}, are a consistent set of S3-generic statistical functions providing fully vectorized statistical operations in R. Specifically, operations such as calculating the mean via the S3 generic \code{fmean()} function are vectorized across columns and groups. They may also involve weights or transformations of the original data. The basic syntax of these functions is
\begin{Code}
FUN(x, g = NULL, [w = NULL], TRA = NULL, [na.rm = TRUE],
  use.g.names = TRUE, drop = TRUE, [nthreads = 1L], ...)
\end{Code}
with arguments \code{x} - data (vector, matrix or data frame-like), \code{g} - groups (atomic vector, list of vectors, or \class{GRP} object), \code{w} - weights, \code{TRA} - transformation, \code{na.rm} - missing values, \code{use.g.names} -  attach group names upon aggregation (if \code{g} is used), \code{drop} - drop dimensions (i.e., simplify to atomic vector if \code{is.null(g)} and \code{x} is matrix or data frame-like), \code{nthreads} - multithreading.\footnote{Not all functions are multithreaded, and parallelism is implemented differently for different functions (detailed in the documentation). The use of single instruction multiple data (SIMD) parallelism in single-threaded mode also implies limited gains from multithreading for simple operations such as \fct{fsum}.} The following examples, taken from the \href{https://sebkrantz.github.io/collapse/articles/collapse_for_tidyverse_users.html#using-the-fast-statistical-functions}{\pkg{collapse} for \pkg{tidyverse} Users} vignette demonstrate their basic usage to calculate (column-wise, grouped, weighted) statistics on different objects. As laid out in the \href{https://sebkrantz.github.io/collapse/articles/collapse_object_handling.html}{vignette on object handling}, statistical functions have basis S3 methods for vectors (\class{default}), \class{matrix}, and \class{data.frame}, which call corresponding \proglang{C} implementations that intelligently preserve object attributes. Thus, the functions can be applied to a broad set of \class{matrix} or \class{data.frame}-based objects without the need to define explicit methods. Users can also directly call the basis methods in case S3 dispatch does not yield the intended outcome. For example, \code{fmean.default(EuStockMarkets)} computes the mean of the entire matrix. %, and attributes are preserved as much as possible ()
<<faststatfun>>=
fmean(mtcars$mpg)
fmean(EuStockMarkets)
fmean(mtcars[5:10])
fmean(mtcars$mpg, w = mtcars$wt)
fmean(mtcars$mpg, g = mtcars$cyl)
fmean(mtcars$mpg, g = mtcars$cyl, w = mtcars$wt)
fmean(mtcars[5:10], g = mtcars$cyl, w = mtcars$wt)
fmean(mtcars$mpg, g = mtcars$cyl, TRA = "fill") |> head(20)
@
%
\subsection{Transformations}
The final example invoking \code{TRA} expands the mean vector to full length, like \code{stats::ave(mtcars$mpg, mtcars$cyl)}, but much faster. The \code{TRA} argument invokes the \fct{TRA} function for column-wise (grouped) replacing and sweeping operations (by reference). Its syntax is
\begin{Code}
TRA(x, STATS, FUN = "-", g = NULL, set = FALSE, ...)
\end{Code}
where \code{STATS} is a vector/matrix/data.frame of statistics used to transform \code{x}. Table~\ref{tab:TRA} lists the 11 possible \code{FUN} operations, toggled using either an integer or string.
%
\begin{table}[h!]
\resizebox{\textwidth}{!}{
  \begin{tabular}{lllll}
  \emph{Int} && \emph{String} && \emph{Description}  \\
  0 && \code{"replace\_na"/"na"}   && replace missing values in \code{x} \\
  1 && \code{"replace\_fill"/"fill"}   && replace data and missing values in \code{x} \\
  2 && \code{"replace"} && replace data but preserve missing values in \code{x} \\
  3 && \code{"-"}   && subtract (i.e., center) \\
  4 && \code{"-+"}  && center on overall average statistic \\
  5 && \code{"/"}   && divide (i.e., scale) \\
  6 && \code{"\%"}     && compute percentages (i.e., divide and multiply by 100) \\
  7 && \code{"+"} && add \\
  8 && \code{"*"} && multiply \\
  9 && \code{"\%\%"} && modulus (i.e., remainder from division by  \code{STATS}) \\
  10 && \code{"-\%\%"} && subtract modulus (i.e., make data divisible by \code{STATS})
  \end{tabular}
}
\caption{\label{tab:TRA} Available \code{FUN} choices in \code{TRA()}.}
\end{table}
%
\code{TRA()} is called internally in the \emph{Fast Statistical Functions}, the \code{TRA} argument is passed to \code{FUN}. Thus \code{fmean(x, g, w, TRA = "-")} is equivalent to \code{TRA(x, fmean(x, g, w), "-", g)}. The \code{set} argument can also be passed to \emph{Fast Statistical Functions} to toggle transformation by reference. The following examples demonstrate how this design allows flexible ad-hoc transformations using \proglang{R}'s built-in \href{https://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/airquality}{\code{airquality} dataset} with daily measurements in New York from May to September 1973.
%
<<aq>>=
fnobs(airquality)
@
This imputes columns \code{Ozone} and \code{Solar.R} by reference using the month median.
<<aq_imp>>=
fmedian(airquality[1:2], airquality$Month, TRA = "replace_na", set = TRUE)
@
This performs different grouped and/or weighted transformations at once.
<<aq_trans>>=
airquality |> fmutate(ozone_deg = Ozone / Temp,
  rad_day = fsum(as.double(Solar.R), Day, TRA = "/"),
  ozone_amed = Ozone > fmedian(Ozone, Month, TRA = "fill"),
  ozone_resid = fmean(Ozone, list(Month, ozone_amed), ozone_deg, "-")
  ) |> head(3)
@
%
\subsection{Grouping objects and optimization}
%
Whereas the \code{g} argument supports ad-hoc grouping with vectors and lists/data frames, the cost of grouping can be optimized by using factors or, even better, \class{GRP} objects, which readily contain all information \pkg{collapse}'s vectorized statistical functions might require to operate across groups. These objects can be created with \code{GRP()}. Its syntax is
\begin{Code}
GRP(X, by = NULL, sort = TRUE, decreasing = FALSE, na.last = TRUE,
  return.groups = TRUE, return.order = sort, method = "auto", ...)
\end{Code}
 The example below creates and displays a \class{GRP} object from 3 columns in \href{https://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/mtcars}{\code{mtcars}}. The \code{by} argument also supports column names or indices, and \code{X} could also be an atomic vector.
%
<<GRP>>=
str(g <- GRP(mtcars, ~ cyl + vs + am))
@
%
\class{GRP} objects make grouped statistical computations in \pkg{collapse} fully programmable. Below, the object is used with the \emph{Fast Statistical Functions} and some utility functions to efficiently aggregate data (with optional frequency weights).
%
<<gcomp>>=
dat <- get_vars(mtcars, c("mpg", "disp")); w <- mtcars$wt;
add_vars(g$groups,
  fmean(dat, g, w, use.g.names = FALSE) |> add_stub("w_mean_"),
  fsd(dat, g, w, use.g.names = FALSE) |> add_stub("w_sd_")) |> head(2)
@
%
Similarly, data can be transformed, here using the S3 generic \fct{fscale} function.
%
<<gtrans>>=
mtcars |> add_vars(fmean(dat, g, w, "-") |> add_stub("w_demean_"),
  fscale(dat, g, w) |> add_stub("w_scale_")) |> head(2)
@
%
This programming access can become very useful. For example, the useR 2022 presentation \href{https://raw.githubusercontent.com/SebKrantz/collapse/master/misc/useR2022\%20presentation/collapse_useR2022_final.pdf}{Slides 18-19} aggregates the \href{https://worldmrio.com/}{EORA Global Supply Chain Database} from the country to the world region level. After defining a single grouping object, a list of value-added shares matrices (VB) and outputs (O) for years 1990-2021, is aggregated with no grouping cost using a single line of code like \code{lapply(VB_list, function(x) x$VB |> fsum(g) |> t() |> fmean(g, x$O) |> t())}. On an M1 Mac using 4 threads, this computation, involving 44.7 million summations and 2.6 million weighted means, takes only 0.33 seconds.\footnote{Another recent example involved numerically optimizing a parameter $a$ in an equation of the form $y_j = \sum_i x_{ij}^a\ \forall j\in J$ where there are $J$ groups (1 million in my case), and the optimal value of $a$ is determined by the proximity of the aggregated vector \textbf{y} to another vector \textbf{z}. Thus each iteration of the numerical routine raises the vector \textbf{x} to a different power ($a$), sums it in 1 million groups ($j$) to generate \textbf{y}, and computes the Euclidean distance to \textbf{z} (using \code{collapse::fdist}). Without grouping objects and vectorization, this would have been difficult to handle within reasonable computing times (of a few seconds on the M1).}
%
\section{Integration with data manipulation functions} \label{sec:integration}
%
\pkg{collapse} also provides a broad set of \href{https://sebkrantz.github.io/collapse/reference/fast-data-manipulation.html}{fast data manipulation functions} familiar to \proglang{R} and \pkg{tidyverse} users, including \fct{fselect}, \fct{fsubset}, \fct{fgroup\_by}, \fct{fsummarise}, \fct{ftransform}, \fct{fmutate}, \fct{across}, \fct{frename}, \fct{fcount}, etc. These are integrated with the \emph{Fast Statistical Functions} to enable vectorized statistical operations in a familiar data frame oriented and \pkg{tidyverse}-like workflow. For example, the following code calculates the mean of columns
%
<<fsummarise>>=
mtcars |> fsubset(mpg > 11) |> fgroup_by(cyl, vs, am) |>
  fsummarise(across(c(mpg, carb, hp), fmean),
             qsec_w_med = fmean(qsec, wt)) |> head(2)
@
%
 \code{mpg}, \code{carb} and \code{hp}, and the weighted mean of \code{qsec}, after subsetting and grouping the data. This code is very fast (especially with many groups) because data does not need to be split by groups at all. There is also no need to call \code{lapply()} inside the \code{across()} statement: \code{fmean.data.frame()} is applied to a subset of the data containing the three columns.\footnote{Internally, the \code{g} argument of the statistical functions is set as a keyword argument by \code{fsummarise/across} and the function is evaluated on a suitable subset of columns. Thus \code{w} becomes the second positional argument...} The \emph{Fast Statistical Functions} also have a method for grouped data, so \code{fsummarise} is not always needed. The following example calculates weighted group means. By default (\code{keep.w = TRUE}) \code{fmean.grouped\_df} also sums the weights in each group.\footnote{\class{grouped\_df} methods in \pkg{collapse} support grouped data created with either \fct{fgroup\_by} or \fct{dplyr::group\_by}. The latter requires an additional \proglang{C} routine to convert the \pkg{dplyr} grouping object to a \class{GRP} object, and is thus less efficient.}
%
<<fsummarise_2>>=
mtcars |> fsubset(mpg > 11, cyl, vs, am, mpg, carb, hp, wt) |>
  fgroup_by(cyl, vs, am) |> fmean(wt) |> head(2)
@
%
\subsection{Vectorizations for advanced tasks} \label{ssec:vfat}
%
\fct{fsummarise} and \fct{fmutate} can also evaluate arbitrary statistical functions in the classical way (split-apply-combine) and handle more complex expressions involving multiple columns and/or functions. However, using any \emph{Fast Statistical Function} causes the whole expression to be vectorized, i.e., evaluated only once and not for every group. This eager vectorization approach enables efficient grouped calculation of more complex statistics. The example below calculates grouped (\code{vs}) bivariate regression slopes (\code{mpg ~ carb}) in a vectorized way.
%
<<flinreg>>=
mtcars |> fgroup_by(vs) |>
 fmutate(dm_carb = fmean(carb, TRA = "-")) |>
 fsummarise(slope = fsum(mpg, dm_carb) %/=% fsum(dm_carb^2))
@
%
Apart from vectorization, this code avoids 3 intermediate copies: (1) \code{fmean(carb, TRA = "-")} avoids an expanded vector of group means, (2) \code{fsum(mpg, dm\_carb)} uses the weights (\code{w}) argument to \fct{fsum} to avoid materializing a multiplication (as in \code{fsum(mpg * dm\_carb)}), and (3) division by reference (\code{\%/=\%}) avoids allocating an additional vector for the final result. Under the hood, the expression boils down to an (expensive) grouping step, 5 allocations (of which 2 full length), and 6 loops in \proglang{C} to calculate the result. Any modern laptop can calculate 1 million regression slopes in less than 1 second like this. Another very neat example, shared by Andrew Ghazi in a recent \href{https://andrewghazi.github.io/posts/collapse\_is\_sick/sick.html}{blog post},\footnote{https://andrewghazi.github.io/posts/collapse\_is\_sick/sick.html} vectorizes an expression to compute the $p$~value across 300k groups for a simulation study, yielding a 70x performance increase over \pkg{dplyr}. \newline

\pkg{collapse} also vectorizes advanced statistics, such as weighted quantiles and modes. The following example calculates a weighted set of summary statistics by groups, with weighted quantiles type 8 following \citet{hyndman1996sample}.\footnote{\pkg{collapse} calculates weighted quantiles by replacing the sample size with the sum of weights and 1 with the minimum non-zero weight in the respective quantile definition. See \href{https://sebkrantz.github.io/collapse/reference/fquantile.html}{fquantile} for more details.} and a weighted maximum mode.\footnote{The weighted maximum mode is the largest element with the maximum sum of weights.}
%
<<wstats>>=
mtcars |> fgroup_by(cyl, vs, am) |>
  fmutate(o = radixorder(GRPid(), mpg)) |>
  fsummarise(mpg_min = fmin(mpg),
             mpg_Q1 = fnth(mpg, 0.25, wt, o = o, ties = "q8"),
             mpg_mean = fmean(mpg, wt),
             mpg_median = fmedian(mpg, wt, o = o, ties = "q8"),
             mpg_mode = fmode(mpg, wt, ties = "max"),
             mpg_Q3 = fnth(mpg, 0.75, wt, o = o, ties = "q8"),
             mpg_max = fmax(mpg)) |> head(3)
@
%
Both weighted mode and quantiles have a sub-column parallel implementation,\footnote{Use \code{set\_collapse(nthreads = \#)} or the \code{nthreads} arguments to \code{fnth/fmedian/fmode} (default 1).} and, as shown above, can also harness an (optional) optimization by computing an overall ordering vector and passing it to each quantile function to avoid repeated partial sorting (using quickselect) of the same elements within groups. For advanced data aggregation, \pkg{collapse} also provides a convenience function, \fct{collap}, which (by default) uses \code{fmean} for numeric and \code{fmode} for non-numeric columns. Below, it aggregates GDP per capita, life expectancy, and country name by World Bank income group, with population weights.\footnote{\href{https://sebkrantz.github.io/collapse/reference/wlddev.html}{\code{wlddev}} is a dataset supplied by \pkg{collapse}, extracted from the World Bank World Development Indicators.} This yields population-weighted statistics, the largest country, and each income group's total population (sum of weights) for each year, preserving (default \code{keep.col.order = TRUE}) the order of columns.
%
<<collap>>=
collap(wlddev, country + PCGDP + LIFEEX ~ year + income, w = ~ POP) |>
  head(4)
@
%
\section{Time series and panel data} \label{sec:ts_ps}
%
\pkg{collapse} also provides a flexible high-performance architecture to perform (time aware) computations on time series and panel series. In particular, the user enjoys great flexibility in deciding the desired degree of indexation and mode of computation. It is possible to apply time series and panel data transformations without any indexation by passing individual and/or time identifiers to the respective functions in an ad-hoc fashion, or by using \class{indexed\_frame} and \class{indexes\_series} classes, which implement full and deep indexation. Table~\ref{tab:TSfun} summarizes \pkg{collapse}'s time series and panel data architecture.
%
\begin{table}[h]
\begin{tabular}{p{\textwidth}}
\emph{Classes, constructors and utilities} \\
\code{findex\_by(), findex(), unindex(), reindex(), timeid(), is\_irregular(), to\_plm()} $+$ S3 methods for \class{indexed\_frame}, \class{indexed\_series} and \class{index\_df} \\\\
\emph{Core time-based functions} \\
\code{flag(), fdiff(), fgrowth(), fcumsum(), psmat()} \\ \code{psacf(), pspacf(), psccf()} \\\\
\emph{Data transformation functions with supporting methods} \\
\code{fscale(), f[hd]between(), f[hd]within()} \\\\
\emph{Data manipulation functions with supporting methods} \\
\code{fsubset(), funique(), roworder[v]()} (internal), \code{na\_omit()} (internal) \\\\
\emph{Summary functions with supporting methods} \\
\code{varying(), qsu()} \\
\end{tabular}
\caption{\label{tab:TSfun} Time series and panel data architecture.}
\end{table}
%
\subsection{Ad-hoc computations}
%
Time series functions such as \fct{fgrowth} (to compute growth rates) are S3 generic and can be applied to most time series classes. In addition to a \code{g} argument for grouped computation, these functions also have a \code{t} argument for indexation. If \code{t} is a plain numeric vector or a factor, it is coerced to integer and interpreted as time steps.\footnote{This is premised on the observation that the most common form of temporal identifier is a numeric variable denoting calendar years. Users need to manually call \code{timeid()} on plain numeric vectors with decimals to yield an appropriate integer representation.} If \code{t} is a numeric time object (e.g., \class{Date}, \class{POSIXct}, etc.), then it is internally passed through \code{timeid()} which computes the greatest common divisor (GCD) and generates an integer time-id. For the GCD approach to work, \code{t} must have an appropriate class, e.g., for monthly/quarterly data, \code{zoo::yearmon()/zoo::yearqtr()} should be used instead of \class{Date} or \class{POSIXct}. % The following provides a basic example:
%
<<ts_example>>=
fgrowth(airmiles) |> round(2)
@
The following code creates an irregular series by removing the 3rd and 15th observation and shows how indexation with the \code{t} argument accounts for this.
%
<<ts_example_ctd>>=
am_ir <- airmiles[-c(3, 15)]
t <- time(airmiles)[-c(3, 15)]
fgrowth(am_ir, t = t) |> round(2)
fgrowth(am_ir, -1:3, t = t) |> head(4)
@
%
For these functions, there also exists shorthands in the form of statistical operators, e.g., \code{L()/D()/G()} are shorthands for \code{flag()/fdiff()/fgrowth()}, which facilitate their use inside formulas and also provide enhanced data frame interfaces for convenient ad-hoc computations. With panel data, \code{t} can be omitted, but this requires sorted data with consecutive groups.\footnote{This is because a group-lag is computed in a single pass, requiring all group elements to be consecutive.}
%
<<example_growth>>=
G(wlddev, c(1, 10), by = POP + LIFEEX ~ iso3c, t = ~ year) |> head(3)
settransform(wlddev, POP_growth = G(POP, g = iso3c, t = year))
@
%
These functions and operators are also integrated with \fct{fsummarise} and \fct{fmutate} for vectorized grouped computations.
%
<<example_growth_continued>>=
wlddev |> fgroup_by(iso3c) |> fselect(iso3c, year, POP, LIFEEX) |>
  fmutate(across(c(POP, LIFEEX), G, t = year)) |> head(2)
@
%
Similarly, functions to scale, center, and average data have groups (\code{g}) and also weights (\code{w}) arguments, and corresponding operators \code{STD(),[HD]W(),[HD]B()} to facilitate ad-hoc transformations. Below, two ways to perform grouped scaling are demonstrated. The operator version is slightly faster and renames the transformed columns by default (\code{stub = TRUE}).
%
<<example_scale_cener>>=
iris |> fgroup_by(Species) |> fscale() |> head(2)
STD(iris, ~ Species) |> head(2)
@
The following example demonstrates a fixed-effects regression \`a la \citet{mundlak1978pooling}.
<<example_reg>>=
lm(mpg ~ carb + B(carb, cyl), data = mtcars) |> coef()
@
\pkg{collapse} also offers higher-dimensional between and within transformations, powered by \proglang{C++} code conditionally imported (and accessed directly) from \pkg{fixest}. The following detrends GDP per Capita and Life Expectancy at Birth using country-specific cubic polynomials.
<<example_HD>>=
HDW(wlddev, PCGDP + LIFEEX ~ iso3c * poly(year, 3), stub = F) |> head(2)
@
%
\subsection{Indexed series and frames}
%
For more complex use cases, indexation is very convenient. \pkg{collapse} supports \pkg{plm}'s \class{pseries} and \class{pdata.frame} classes through dedicated methods. Flexibility and performance considerations lead
to the creation of new classes \class{indexes\_series} and \class{indexed\_frame} which inherit from the former. Any data frame-like object can be an \class{indexed\_frame} with any number of individual and/or time identifiers (e.g., an indexed \class{data.table} is fully functional for other operations). The technical implementation of these classes is described in the \href{https://sebkrantz.github.io/collapse/articles/collapse_object_handling.html#class-agnostic-grouped-and-indexed-data-frames}{vignette on object handling} and, in more detail, in the \href{https://sebkrantz.github.io/collapse/reference/indexing.html}{documentation}. The basic syntax is:
%
\begin{Code}
data_ix <- findex_by(data, id1, ..., time)
data_ix$indexed_series; with(data, indexed_series)
index_df <- findex(data_ix)
\end{Code}
%
Data can be indexed using one or more indexing variables. Unlike \class{pdata.frame}, an \\ \class{indexed\_frame} is a deeply indexed structure, i.e., every series inside the frame is already an \class{indexes\_series} and contains, in its \class{index\_df} attribute, an external pointer to the \class{index\_df} attribute of the frame (to avoid duplication in memory). A comprehensive set of \href{https://sebkrantz.github.io/collapse/reference/indexing.html}{methods for subsetting and manipulation}, and applicable \class{pseries} and \class{pdata.frame} methods for time series and transformation functions like \code{flag()/L()}, ensure that these objects behave in a time-/panel-aware manor in any caller environment (created by \fct{with}, \fct{lm} etc.). % A basic demonstration with World Bank panel data showcases the flexibility of these classes.
Indexation can be undone using \code{unindex()} and redone with \code{reindex()} and a suitable \class{index\_df}. \class{indexes\_series} can be atomic vectors or matrices (including objects such as \class{ts} or \class{xts}) and can also be created directly using \code{reindex()}.
%
\begin{Code}
data <- unindex(data_ix)
data_ix <- reindex(data, index = index_df)
indexed_series <- reindex(vec/mat, index = vec/index_df)
\end{Code}
%
An example using the \code{wlddev} data follows:
<<indexing>>=
wldi <- wlddev |> findex_by(iso3c, year)
wldi |> fsubset(-3, iso3c, year, PCGDP:POP) |> G() |> head(4)
@
The index statistics are: \code{[N. ids] | [N. periods (total periods: (max-min)/GCD)]}. This creates an \class{indexes\_series} of life expectancy and demonstrates its properties:
<<indexing_2>>=
LIFEEXi <- wldi$LIFEEX; str(LIFEEXi, width = 70, strict = "cut")
c(is_irregular(LIFEEXi), is_irregular(LIFEEXi[-5]))
G(LIFEEXi[c(1:5, 7:10)])
@
The transformation and estimation below demonstrate the deep indexation of \\ \class{indexed\_frame}'s, allowing correct computations in arbitrary data masking environments.
<<indexing_3>>=
settransform(wldi, PCGDP_ld = Dlog(PCGDP))
lm(D(LIFEEX) ~ L(PCGDP_ld, 0:5) + B(PCGDP_ld), wldi) |>
  summary() |> coef() |> round(3)
@
The above example could also have been executed in one line as \code{lm(D(LIFEEX) ~ } \\ \code{L(Dlog(PCGDP), 0:5) + B(Dlog(PCGDP)), wldi)}, log-differencing \code{PCGDP} twice. \newline

In comparison with existing solutions, the flexibility of this architecture is new to the \proglang{R} ecosystem: A \class{pdata.frame} or \class{fixest\_panel} only works inside \pkg{plm}/\pkg{fixest} estimation functions.\footnote{And, in the case of \pkg{fixest}, inside \pkg{data.table} due to dedicated methods.} Time series classes like \class{xts} and \class{tsibble} also do not provide deeply indexed structures for time series operations or native handling of irregularity in basic operations. \class{indexed\_series} and \class{indexed\_frame}, on the other hand, work anywhere and can be superimposed on any suitable object (such as \class{sf} to create a spatiotemporal panel), as long as \pkg{collapse}'s functions (\code{flag()/L()} etc.) are used to perform the time-based computations. The \class{index\_df} attached to these objects can be used with other general tools such as \code{collapse::BY()} to perform grouped computations using 3rd-party functions. An example of calculating a 5-year rolling average is given below. Last but not least, the performance of these classes is second to none, as demonstrated in the useR 2022 presentation \href{https://raw.githubusercontent.com/SebKrantz/collapse/master/misc/useR2022\%20presentation/collapse_useR2022_final.pdf}{on slide 40}.
%
<<rollmean>>=
BY(LIFEEXi, findex(LIFEEXi)$iso3c, data.table::frollmean, 5) |> head(10)
@
%
\section{Table joins and pivots} \label{sec:join_pivot}
%
While \pkg{collapse} has a broad set of \href{https://sebkrantz.github.io/collapse/reference/fast-data-manipulation.html}{data manipulation functions}, its implementations of table \href{https://sebkrantz.github.io/collapse/reference/join.html}{joins} and \href{https://sebkrantz.github.io/collapse/reference/pivot.html}{pivots} is particularly noteworthy since they offer several new features, including rich verbosity for table joins, pivots supporting variable labels, and 'recast' pivots. Both implementations provide outstanding computational performance and memory efficiency.
%
\subsection{Joins}
%
Compared to commercial environments such as \proglang{STATA}, the implementation of joins in most open-source software, including \proglang{R}, is non-verbose, i.e., provides no information on how many and which records were joined from both tables. This is somewhat unsatisfying and often provokes manual efforts to validate the join operation. \code{collapse::join} provides a rich set of options to make table join operations intelligible. Its syntax is:
\begin{Code}
join(x, y, on = NULL, how = "left", suffix = NULL, validate = "m:m",
  multiple = FALSE, sort = FALSE, keep.col.order = TRUE,
  drop.dup.cols = FALSE, verbose = 1, column = NULL, attr = NULL, ...)
\end{Code}
By default (\code{verbose = 1}), it prints information about the join operation and number of records joined. Users can request the generation of a \code{.join} column (\code{column = "name"/TRUE}), akin to \proglang{STATA}'s \code{\_merge} column, indicating the origin of records in the joined table.
%
<<joins>>=
df1 <- data.frame(id1 = c(1, 1, 2, 3), id2 = c("a", "b", "b", "c"),
  name = c("John", "Jane", "Bob", "Carl"), age = c(35, 28, 42, 50))
df2 <- data.frame(id1 = c(1, 2, 3, 3), id2 = c("a", "b", "c", "e"),
  salary = c(60000, 55000, 70000, 80000),
  dept = c("IT", "Marketing", "Sales", "IT"))
join(df1, df2, on = c("id1", "id2"), how = "full", column = TRUE)
@
%
An alternative to the join column is to request an attribute (\code{attr = "name"/TRUE}) that also summarizes the join operation, including the output of \fct{fmatch} (the workhorse of \fct{join} if \code{sort = FALSE}). Users can also invoke the \code{validate} argument to check the uniqueness of the join keys in either table: passing a '1' for a non-unique key produces an error.
%
<<joins_2>>=
join(df1, df2, on = c("id1", "id2"), validate = "1:1", attr = "join") |>
  attr("join") |> str(width = 70, strict = "cut")
@
%
A few further particularities are worth highlighting. First, \code{collapse::join} is also class-agnostic and preserves the attributes of \code{x}. It supports 6 different join operations (\code{"left"}, \code{"right"}, \code{"inner"}, \code{"full"}, \code{"semi"}, or \code{"anti"}) and defaults to \code{"left"}, so the default behavior simply adds columns to \code{x}. By default (\code{sort = FALSE}), the order of rows in \code{x} is also preserved. Setting \code{sort = TRUE} sorts all records in the joined table by the keys.\footnote{This is done using a separate sort-merge-join algorithm, so it is faster than performing a hash join (using \fct{fmatch}) followed by sorting, particularly if the data is already sorted on the keys. } Additionally, by default (\code{multiple = FALSE}), only the first matches from \code{y} are joined to avoid silent cartesian duplication of records. In multi-match settings, this will be reflected by few records from \code{y} being used. \fct{fmatch} also has a built-in overidentification check, which issues a warning if more key columns than necessary to identify the records are provided:
%
<<join_3>>=
df2$name = df1$name
join(df1, df2) |> capture.output(type = "m") |>
  strwrap(77) |> cat(sep = "\n")
@
%
% This warning can be silenced by passing \code{overid = 2} or restricting the join columns using \code{on}.
A final noteworthy feature is the handling of duplicate non-id columns in both tables:
%
<<join_4>>=
join(df1, df2, on = c("id1", "id2"))
@
%
By default (\code{suffix = NULL}), \fct{join} extracts the name of the \code{y} table and appends \code{y}-columns with it. \code{x}-columns are not renamed. This is congruent to the principle of adding columns to \code{x} and altering this table as little as possible. Another option, \code{drop.dup.cols = "x"/"y"}, can be used to simply drop duplicate columns from \code{x} or \code{y} before the join operation.
%
\subsection{Pivots}
%
The reshaping/pivoting functionality of both commercial and open source software is also unsatisfying for complex datasets such as surveys or disaggregated production, trade, or financial sector data, where variable names resemble codes and variable labels are essential to making sense of the data. Such datasets can presently only be reshaped by losing these labels or additional manual efforts to retain them. Modern \proglang{R} packages also offer different functions for different reshaping operations, such as \code{data.table::melt/tidyr::pivot_longer} to combine columns and \code{data.table::dcast/tidyr::pivot_wider} to expand them, requiring users to learn both. Since the depreciation of \pkg{reshape(2)} \citep{rreshape2}, there is also no modern replacement for \code{reshape2::recast()}, requiring \proglang{R} users to consecutively call two reshaping functions, incurring a high cost in terms of syntax and memory. \newline

\code{collapse::pivot} provides a modern class-agnostic implementation of reshaping for \proglang{R} that addresses these shortcomings: it has a single intuitive syntax to perform 'longer', 'wider', and 'recast' pivots, and supports complex labelled data without loss of information. Its syntax is:
%
\begin{Code}
pivot(data, ids = NULL, values = NULL, names = NULL, labels = NULL,
  how = "longer", na.rm = FALSE, factor = c("names", "labels"),
  check.dups = FALSE, nthreads = 1, fill = NULL, drop = TRUE,
  sort = FALSE, transpose = FALSE)
\end{Code}
%
The demonstration below employs a generated dataset about fruits. We could equivalently think about a survey with households and individuals, or sectors and firms. Variable labels are stored in \code{attr(column, "label")}. The \href{https://sebkrantz.github.io/collapse/reference/pivot.html#ref-examples}{documentation} provides more elaborate examples.
%
<<pivots>>=
data <- data.frame(type = rep(c("A", "B"), each = 2),
  type_name = rep(c("Apples", "Bananas"), each = 2),
  id = rep(1:2, 2), r = abs(rnorm(4)), h = abs(rnorm(4)*2))
setrelabel(data, id = "Fruit Id", r = "Fruit Radius", h = "Fruit Height")
print(data)
vlabels(data)
@
%
To reshape this dataset into a longer format, it suffices to call \code{pivot(data, ids = c(...))}. If \code{labels = "lab_name"} is specified, variable labels are saved to an additional column named \code{lab\_name}. In addition, \code{names = list(variable = "var_name", value = "val_name")} can be passed to assign alternative names to the \code{variable} and \code{value} columns, respectively.
%
<<pivot_longer>>=
(dl <- pivot(data, ids = c("type", "type_name", "id"), labels = "label"))
vlabels(dl)
@
%
\fct{pivot} only requires essential information and intelligently guesses the rest. For example, the same result could have been obtained by \code{pivot(data, values = c("r", "h"), labels = "label")}. An exact reverse operation can also be performed by specifying as little as \code{pivot(dl, labels = "label", how = "w")}. \newline

The second option is a wider pivot with \code{how = "wider"}. Here, \code{names} and \code{labels} can be used to select columns containing the names of new columns and their labels.\footnote{multiple columns with names and labels could be selected, which would be combined using \code{"\_"} for names and \code{" - "} for labels.} Note how the labels are combined with existing labels such that also this operation is without loss of information. It is, however, a destructive operation, i.e., with 2 or more columns selected through \code{values}, \fct{pivot} is not able to reverse it. Further arguments like \code{na.rm}, \code{fill}, \code{sort}, and \code{transpose} can be used to control the casting process.
%
<<pivot_wider>>=
(dw <- pivot(data, "id", names = "type", labels = "type_name", how = "w"))
namlab(dw)
@
%
For the recast pivot (\code{how = "recast"}), unless a column named \code{variable} exists in the data, the source and (optionally) destination of variable names need to be specified using a list passed to \code{names}, and similarly for \code{labels}. Again, taking along labels is optional, and omitting either the list's \code{from} or \code{to} elements will omit the respective operations.
%
<<pivot_recast>>=
(dr <- pivot(data, ids = "id", names = list(from = "type"),
             labels = list(from = "type_name", to = "label"), how = "r"))
vlabels(dr)
@
%
As with the other pivots, this operation does not incur any loss of information. A suitable reverse operation also exists: \code{pivot(dr, "id", names = list(to = "type"), labels = list(from = "label", to = "type_name"), how = "r")}. More features of \fct{pivot} are demonstrated in the \href{https://sebkrantz.github.io/collapse/reference/pivot.html#ref-examples}{documentation examples}. Notably, it is also possible to perform longer and recast pivots without id variables. The recast pivot without ids resembles a generalization of \code{data.table::transpose()}, albeit slightly less efficient.
%
\section{List processing} \label{sec:list_proc}
%
Often in programming, nested structures are needed. A typical use case involves running statistical procedures for multiple configurations of variables and parameters and saving multiple objects (such as a model object, performance statistics, and predictions) in a list. Nested data is also often the result of web scraping or web APIs. A typical use case in development involves serving different data according to user choices, e.g., in response to nested user inputs in shiny apps. Except for certain recursive functions found in packages such as \pkg{purr}, \pkg{tidyr}, or \pkg{rrapply}, \proglang{R} lacks a general recursive toolkit to create, query, and tidy nested data. \pkg{collapse}'s \href{https://sebkrantz.github.io/collapse/reference/list-processing.html}{list processing functions} attempt to provide a basic toolkit. \newline

To create nested data, \fct{rsplit} generalizes \fct{split} and (recursively) splits up data frame-like objects into a (nested) list.
%
<<list_proc>>=
(dl <- mtcars |> rsplit(mpg + hp + carb ~ vs + am)) |> str(max.level = 2)
@
%
If a nested structure is not wanted, argument \code{flatten = TRUE} lets \fct{rsplit} operate like a faster version of \fct{split}. With a single column on the LHS of the formula, the default (\code{simplify = TRUE}) returns a nested list of atomic vectors. Having created a nested list, \fct{rapply2d} is used to fit a linear model on each frame,\footnote{\fct{rapply2d} is just a recursive wrapper around \fct{lapply}, with different defaults than \fct{rapply}. Notably, by default, it excludes data frames from being considered as sub-lists and does not simplify the result.} followed by \fct{get\_elem} to obtain the coefficient matrices. \fct{get\_elem} offers several options for filtering lists but, by default, simplifies the list tree as much as possible while maintaining existing hierarchies. In this case, it returns the same nested list with coefficient matrices in all final nodes.
%
<<list_proc_2>>=
nest_lm_coef <- dl |> rapply2d(lm, formula = mpg ~ .) |>
  rapply2d(summary, classes = "lm") |> get_elem("coefficients")
nest_lm_coef |> str(give.attr = FALSE, strict = "cut")
@
%
At last, \fct{unlist2d} is applied to unlist the nested list to a data frame. This function can create a data frame (or \class{data.table}) representation of any nested list containing data using recursive row-binding and coercion operations while generating (optional) id variables representing the list tree and (optionally) saving row names of matrices or data frames.
%
<<list_proc_3>>=
nest_lm_coef |> unlist2d(c("vs", "am"), row.names = "variable") |> head(2)
@
%
This example does not represent an optimal workflow for this specific task\footnote{A better way of achieving the same result would be \code{mtcars |> fgroup\_by(vs, am) |> fsummarise(qDF(lmtest::coeftest(lm(mpg ~ hp + carb)), "variable"))}.} but exemplifies the power of these tools to create, query, and combine nested data in very general ways.
  % I use it extensively to fetch and compare different statistics from (sometimes large) nested lists with different statistical model outputs.
\pkg{collapse}'s list processing toolkit provides further useful functions such as \fct{t\_list} to turn lists of lists inside out, \fct{has\_elem} to check for the existence of elements, \fct{ldepth} to return the maximum level of recursion, and \fct{is\_unlistable} to check whether a list has atomic elements in all final nodes. A non-recursive and class-agnostic \fct{rowbind} function also exists to efficiently bind lists of data frame-like objects (like \code{data.table::rbindlist()}).
%
\section{Summary statistics} \label{sec:summ_stat}
%
\pkg{collapse}'s \href{https://sebkrantz.github.io/collapse/reference/summary-statistics.html}{summary statistics functions} offer a parsimonious and powerful toolset to examine complex datasets. A particular focus has been on providing tools for examining longitudinal (panel) data. Recall the indexed world development panel (\code{wldi}) from Section~\ref{sec:ts_ps}. The function \fct{varying} indicates which of these variables are time-varying:
%
<<prep, include=FALSE, echo=FALSE>>=
wldi <- wldi[1:13]
# options(width = 77)
@
<<varying_1>>=
varying(wldi)
varying(wldi, any_group = FALSE) |> head(3)
@
%
Country-variance can be examined using \code{varying(wldi, effect = "year")}. For non-indexed data, \fct{varying} also has a \code{g} argument. A related exercise is to decompose the variance of a panel series into a component due to variation between countries and one capturing variance within countries over time. Using the \code{W()/B()} operators and the \code{LIFEEXi} \class{indexed\_series} from Section~\ref{sec:ts_ps}, this is easily demonstrated:
%
<<pdec>>=
all.equal(fvar(W(LIFEEXi)) + fvar(B(LIFEEXi)), fvar(LIFEEXi))
@
%
The function \fct{qsu} (quick-summary) provides an efficient method to approximately compute this decomposition, considering the group-means instead of the between transformation\footnote{This is more efficient and equal to using the between transformation if the panel is balanced.} and adding the mean back to the within transformation to preserve the scale of the data.
%
<<qsu_1>>=
qsu(LIFEEXi)
@
%
This decomposition shows more variation in life expectancy between countries than within countries over time. It can also be computed for different subgroups, such as OECD members and non-members, and with sampling weights, such as population. \fct{qsu} can also return Pearson's measures of higher-order statistics.
%
<<qsu_2>>=
qsu(LIFEEXi, g = wlddev$OECD, w = wlddev$POP, higher = TRUE) |> aperm()
@
%
The output shows that the variation in life expectancy is significantly larger for non-OECD countries and that for these countries, the between- and within-country variation is approximately equal in magnitude.\footnote{\fct{qsu} also has a convenient formula interface to perform these transformations in an ad-hoc fashion, e.g., the above can be obtained using \code{qsu(wlddev, LIFEEX ~ OECD, ~ iso3c, ~POP, higher = TRUE)}, without prior indexation.} For more detailed (grouped, weighted) statistics, \fct{descr} provides a rich statistical description of variables in a dataset.
%
<<descr>>=
descr(wlddev, LIFEEX ~ OECD, w = ~ replace_na(POP))
@
<<set_width, echo=FALSE>>=
# options(width = 80)
@

%
While \fct{descr} does not support panel-variance decompositions like \fct{qsu}, it also computes detailed (grouped, weighted) frequency tables for categorical data and is thus very utile with complex surveys. A \code{stepwise} argument toggles describing one variable at a time, allowing users to naturally 'click-through' a large dataset rather than printing a massive output to the console. More details and examples are in the \href{https://sebkrantz.github.io/collapse/reference/descr.html}{documentation}. Both \fct{qsu} and \fct{descr} provide an \code{as.data.frame()} method for efficient tidying and further analysis. \newline

A final noteworthy function from \pkg{collapse}'s descriptive statistics toolkit is \fct{qtab}, an enhanced drop-in replacement for \code{base::table}. It is enhanced both in a statistical and computational sense, providing a remarkable performance boost, an option (\code{sort = FALSE}) to preserve the first-appearance-order of vectors being cross-tabulated, support for frequency weights (\code{w}), and the ability to compute different statistics representing table entries using these weights - vectorized when using \emph{Fast Statistical Functions}, as demonstrated below.
%
<<qtab>>=
library("magrittr")
wlda15 <- wlddev |> fsubset(year >= 2015) |> fgroup_by(iso3c) |> flast()
wlda15 %$% qtab(OECD, income)
@
This shows the total population (latest post-2015 estimates) in millions.
<<qtab_2>>=
wlda15 %$% qtab(OECD, income, w = POP) %>% divide_by(1e6)
@
This shows the average life expectancy in years. The use of \fct{fmean} toggles an efficient vectorized computation of the table entries (i.e., \fct{fmean} is only called once).
<<qtab_3>>=
wlda15 %$% qtab(OECD, income, w = LIFEEX, wFUN = fmean) %>% replace_na(0)
@
Finally, this calculates a population-weighted average of life expectancy in each group.
<<qtab_4>>=
wlda15 %$% qtab(OECD, income, w = LIFEEX, wFUN = fmean,
                wFUN.args = list(w = POP)) %>% replace_na(0)
@
%
\class{qtab} objects inherit the \class{table} class, thus all \class{table} methods apply. Apart from the above functions, \pkg{collapse} also provides functions \pkg{pwcor}, \pkg{pwcov}, \pkg{pwnobs} for convenient (pairwise, weighted) correlations, covariances, and observations counts, and also functions \pkg{psacf}, \pkg{pspacf} and \pkg{psccf} for auto- and cross-covariance and correlation function estimation on panel series.
%
\section{Global options} \label{sec:glob_opt}
%
\pkg{collapse} is \href{https://sebkrantz.github.io/collapse/reference/collapse-options.html}{globally configurable} to an extent few packages are: the default value of key function arguments governing the behavior of its algorithms, and the exported namespace, can be adjusted interactively through the \fct{set\_collapse} function. These options are saved in an internal environment called \code{.op} (for safety and performance reasons) visible in the \href{https://sebkrantz.github.io/collapse/reference/fmean.html}{documentation of some functions}. Its contents can be accessed using \fct{get\_collapse}. \newline

The current set of options comprises the default behavior for missing values (\code{na.rm} arguments in all statistical functions and algorithms), sorted grouping (\code{sort}), multithreading and algorithmic optimizations (\code{nthreads}, \code{stable.algo}), presentational settings (\code{stub}, \code{digits}, \code{verbose}), and, surpassing all else, the package namespace itself (\code{mask}, \code{remove}). \newline

As evident from previous sections, \pkg{collapse} provides performance-improved or otherwise enhanced versions of functionality already present in base R (like the \emph{Fast Statistical Functions}, \fct{funique}, \fct{fmatch}, \fct{fsubset}, \fct{ftransform}, etc.) and other packages (esp. \pkg{dplyr} \citep{rdplyr}: \fct{fselect}, \fct{fsummarise}, \fct{fmutate}, \fct{frename}, etc.). The objective of being namespace compatible warrants such a naming convention, but this has a syntactical cost, particularly when \pkg{collapse} is the primary data manipulation package. \newline

To reduce this cost, \pkg{collapse}'s \code{mask} option allows masking existing \proglang{R} functions with the faster \pkg{collapse} versions by creating additional functions in the namespace and instantly exporting them. All \pkg{collapse} functions starting with 'f' can be passed to the option (with or without the 'f'), e.g., \code{set\_collapse(mask = c("subset", "transform"))} creates \code{subset <- fsubset} and \code{transform <- ftransform} and exports them. Special functions are \code{"n"}, \code{"table"/"qtab"}, and \code{"\%in\%"}, which create \code{n <- GRPN} (for use in \code{(f)summarise}/\code{(f)mutate}), \code{table <- qtab}, and replace \code{\%in\%} with a fast version using \code{fmatch}, respectively. There are also several \href{https://sebkrantz.github.io/collapse/reference/collapse-options.html}{convenience keywords to mask related groups of functions}. The most powerful of these is \code{"all"}, which masks all f-functions $+$ specials, as shown below.
%
\begin{Code}
set_collapse(mask = "all", na.rm = FALSE, sort = FALSE, nthreads = 4)
wlddev |> subset(year >= 1990 & is.finite(GINI)) |>
  group_by(year) |>
  summarise(n = n(), across(PCGDP:GINI, mean, w = POP))
with(mtcars, table(cyl, vs, am))
sum(mtcars)
diff(EuStockMarkets)
mean(num_vars(iris), g = iris$Species)
unique(wlddev, cols = c("iso3c", "year"))
range(wlddev$date)
wlddev |> index_by(iso3c, year) |>
  mutate(PCGDP_lag = lag(PCGDP),
         PCGDP_diff = PCGDP - PCGDP_lag,
         PCGDP_growth = growth(PCGDP)) |> unindex()
\end{Code}
%
The above is now 100\% \pkg{collapse} code. Similarly, using this option, all code in this article could have been written without f-prefixes. Thus, \pkg{collapse}, together with namespace masking, is able to provide a fast and syntactically clean experience of \proglang{R} - without the need to even restart the session. Masking is completely interactive and reversible within the active session: calling \code{set\_collapse(mask = NULL)} instantly removes the additional functions. Option \code{remove} can further be used to remove any \pkg{collapse} function from the list of exported functions, allowing manual conflict management. Function \code{fastverse::fastverse\_conflicts()} from the related \href{https://fastverse.github.io/fastverse/}{\pkg{fastverse} project}\footnote{Website: https://fastverse.github.io/fastverse/} can be used to display namespace conflicts with \pkg{collapse}. Invoking either \code{mask} or \code{remove} detaches \pkg{collapse} and reattaches it at the top of the search path, letting its namespace to take precedence over other packages.
%
\section{Benchmark} \label{sec:bench}
%
This section offers a small benchmark to demonstrate that \pkg{collapse} provides best-in-\proglang{R} performance for many basic statistical and data manipulation tasks. They are executed on an Apple M1 MacBook Pro with 16 GB unified memory. The \href{https://duckdblabs.github.io/db-benchmark/}{DuckDB Benchmarks} compare more software packages on larger datasets, using a large server with many (slow) cores.\footnote{A lot may be said about benchmarking \pkg{collapse}, which would be beyond the scope of this article. Users should note, however, that its defaults (\code{na.rm = TRUE, sort = TRUE, stable.algo = TRUE, nthreads = 1}) cater to convenience rather than maximum performance. For maximum performance, set these 3 settings to \code{FALSE} and increase the number of threads. To also provide a minimalistic guide for \proglang{R} users seeking to understand the relative performance of \pkg{collapse} and \pkg{data.table}, reflecting current (spring 2024) developments: \pkg{collapse} has highly efficient algorithms for grouping and computing statistics, but presently does not provide sub-column level parallel grouping architecture. Simple statistics like \code{fmean()} are parallelized across columns and perform grouped computations in a single pass. More complex ones \code{fmedian(), fmode()} have group-level parallelism. \pkg{data.table}, on the other hand, has sub-column parallel grouping and also group-level parallel implementations for simple statistics such as \code{mean()}, but no parallelism for complex statistics such as \code{median()}. \pkg{data.table}'s \href{https://rdatatable.gitlab.io/data.table/reference/datatable-optimize.html}{GForce optimization} also only applies to simple statistics, not complex expressions or weighted statistics - as can be vectorized using \emph{Fast Statistical Functions} in \pkg{collapse}. Thus, if your data is moderately sized ($\leq$100mio. obs.), you have more than 1 column to compute on, you want to do complex statistical things, or if your processor is very fast (high single core speed), \pkg{collapse} is a great choice. On the other hand, if your data is really long ($>$100mio. obs.), you have only a few columns to compute on, you are computing simple statistics that \pkg{data.table} optimizes, and you have massive parallel compute, then \pkg{data.table} is a great choice. My recommendation: use both, just need to call \code{library(fastverse)}. Finally, let me note that \pkg{polars} uses optimized memory buffers based on \href{https://arrow.apache.org/}{Apache Arrow}, \href{https://pola.rs/posts/i-wrote-one-of-the-fastest-dataframe-libraries/}{multithreaded hash-based grouping, SIMD instructions and multithreading at the group-level}, and a \href{https://pola.rs/posts/polars_birds_eye_view/}{query optimizer} - all implemented in \proglang{Rust}, a thread-safe programming language. While some of these parallel algorithms could be ported to \pkg{collapse}, this is more challenging since \proglang{C}, and particularly \proglang{R}'s \proglang{C} API, is not thread safe - and it would still be lacking the benefits of Arrow memory buffers. At core, \proglang{R} is a 30-year old statistical language and not intended to work like an optimized database. \pkg{collapse} seamlessly integrates with \proglang{R}'s data structures; \pkg{polars}, at present, has nothing to do with them (and is therefore also not part of this benchmark).}
%
<<bench, cache=TRUE>>=
setDTthreads(4)
set_collapse(na.rm = FALSE, sort = FALSE, nthreads = 4)
set.seed(101)
m <- matrix(rnorm(1e7), ncol = 1000)
data <- qDT(replicate(100, rnorm(1e5), simplify = FALSE))
g <- sample.int(1e4, 1e5, TRUE)

microbenchmark(R = colMeans(m),
               Rfast = Rfast::colmeans(m, parallel = TRUE, cores = 4),
               collapse = fmean(m))
microbenchmark(R = rowsum(data, g, reorder = FALSE),
               data.table = data[, lapply(.SD, sum), by = g],
               collapse = fsum(data, g))
add_vars(data) <- g
microbenchmark(data.table = data[, lapply(.SD, median), by = g],
               collapse = data |> fgroup_by(g) |> fmedian())
d <- data.table(g = unique(g), x = 1, y = 2, z = 3)
microbenchmark(data.table = d[data, on = "g"],
               collapse = join(data, d, on = "g", verbose = 0))
microbenchmark(data.table = melt(data, "g"),
               collapse = pivot(data, "g"))
settransform(data, id = rowid(g))
cols = grep("^V", names(data), value = TRUE)
microbenchmark(data.table = dcast(data, g ~ id, value.var = cols),
          collapse = pivot(data, ids = "g", names = "id", how = "w"))
@
%
The benchmark below further shows that \pkg{collapse} provides faster algorithms for basic computationally intensive operations such as unique values and matching. These algorithms power much of its functionality, such as efficient factor generation with \fct{qF}, cross-tabulation with \fct{qtab}, \fct{join}'s, \fct{pivot}'s, etc.
%
<<bench_2, cache=TRUE>>=
set.seed(101)
g_int <- sample.int(1e3, 1e7, replace = TRUE)
char <- c(letters, LETTERS, month.abb, month.name)
char <- outer(char, char, paste0)
g_char <- sample(char, 1e7, replace = TRUE)
microbenchmark(base_int = unique(g_int), collapse_int = funique(g_int),
            base_char = unique(g_char), collapse_char = funique(g_char))
microbenchmark(base_int = match(g_int, 1:1000),
               collapse_int = fmatch(g_int, 1:1000),
               base_char = match(g_char, char),
               data.table_char = chmatch(g_char, char),
               collapse_char = fmatch(g_char, char), times = 10)
@
%
Apart from the raw algorithmic efficiency demonstrated here, \pkg{collapse} is often more efficient than other solutions by simply doing less. For example, if grouping columns are factor variables, \pkg{collapse}'s algorithms in \code{funique()}, \code{group()} or \code{fmatch()}, etc., use the values as hashes without checking for collisions. Similarly, if data is already sorted/unique, it is directly returned by functions like \code{roworder()}/\code{funique()}.

\newpage
%% -- Summary/conclusions/discussion -------------------------------------------

\section{Conclusion} \label{sec:conclusion}

It is coming close to 4 years since the first CRAN release of \pkg{collapse} in March 2020, and since then, the package has grown and matured considerably. At the time of writing this article in early 2024, it has been downloaded $>$1.5 million times off CRAN. In this article, I have articulated key ideas and design principles and demonstrated some core features of the package. In summary, my work with \proglang{R} as an applied economist has led me to believe that there should be a new foundation package for statistical computing and data manipulation in \proglang{R} that is statistically advanced, class-agnostic, flexible, fast, lightweight, stable, and able to manipulate complex scientific data with ease. \pkg{collapse} is my attempt at providing such a package, and the feedback I have received over the years, particularly from users in academia, government, and international organizations, is a strong indication that I have responded to a need felt in larger parts of the \proglang{R} community. As mentioned, a single article cannot comprehensively introduce \pkg{collapse}, but there is a modern \href{https://sebkrantz.github.io/collapse/index.html}{website} with comprehensive \href{https://sebkrantz.github.io/collapse/articles/collapse_documentation.html}{documentation resources}.


%% -- Optional special unnumbered sections -------------------------------------

\section*{Computational details}
The results in this paper were obtained using \proglang{R} \citep{R} 4.3.0 with \pkg{collapse} 2.0.10, \pkg{data.table} 1.15.0, \pkg{Rfast} 2.1.0, \pkg{fixest} 0.11.3, \pkg{magrittr} \citep{rmagrittr} 2.0.3 and \pkg{microbenchmark} \citep{rmicrobenchmark} 1.4.10. All packages used are available from the Comprehensive \proglang{R} Archive Network (CRAN) at https://CRAN.R-project.org/. The benchmark was run on an Apple M1 MacBook Pro (2020) with 16GB unified memory. Packages were compiled from source using Homebrew Clang version 16.0.4 with OpenMP enabled and the -O3 optimization flag.

\section*{Acknowledgments}

The source code of \pkg{collapse} has been heavily inspired by (and partly copied from) \pkg{data.table} (Matt Dowle and Arun Srinivasan), \proglang{R}'s source code (R Core Team and contributors worldwide), the \pkg{kit} package (Morgan Jacob), and \pkg{Rcpp} (Dirk Eddelbuettel). Packages \pkg{plm} (Yves Croissant, Giovanni Millo, and Kevin Tappe) and \pkg{fixest} (Laurent Berge) have also provided a lot of inspiration (and a port to its demeaning algorithm in the case of \pkg{fixest}). I also thank many people from diverse fields for helpful answers on Stackoverflow and many other people for encouragement, feature requests, and helpful issues and suggestions.
% \begin{leftbar}
% All acknowledgments (note the AE spelling) should be collected in this
% unnumbered section before the references. It may contain the usual information
% about funding and feedback from colleagues/reviewers/etc. Furthermore,
% information such as relative contributions of the authors may be added here
% (if any).
% \end{leftbar}

%% -- Bibliography -------------------------------------------------------------
%% - References need to be provided in a .bib BibTeX database.
%% - All references should be made with \cite, \citet, \citep, \citealp etc.
%%   (and never hard-coded). See the FAQ for details.
%% - JSS-specific markup (\proglang, \pkg, \code) should be used in the .bib.
%% - Titles in the .bib should be in title case.
%% - DOIs should be included where available.

\newpage

\bibliography{refs}

%% -- Appendix (if any) --------------------------------------------------------
%% - After the bibliography with page break.
%% - With proper section titles and _not_ just "Appendix".

\newpage
%
% \begin{appendix}
%
% \section{More technical details} \label{app:technical}
%
% \begin{leftbar}
% Appendices can be included after the bibliography (with a page break). Each
% section within the appendix should have a proper section title (rather than
% just \emph{Appendix}).
%
% For more technical style details, please check out JSS's style FAQ at
% \url{https://www.jstatsoft.org/pages/view/style#frequently-asked-questions}
% which includes the following topics:
% \begin{itemize}
%   \item Title vs.\ sentence case.
%   \item Graphics formatting.
%   \item Naming conventions.
%   \item Turning JSS manuscripts into \proglang{R} package vignettes.
%   \item Trouble shooting.
%   \item Many other potentially helpful details\dots
% \end{itemize}
% \end{leftbar}
%
%
% \section[Using BibTeX]{Using \textsc{Bib}{\TeX}} \label{app:bibtex}
%
% \begin{leftbar}
% References need to be provided in a \textsc{Bib}{\TeX} file (\code{.bib}). All
% references should be made with \verb|\cite|, \verb|\citet|, \verb|\citep|,
% \verb|\citealp| etc.\ (and never hard-coded). This commands yield different
% formats of author-year citations and allow to include additional details (e.g.,
% pages, chapters, \dots) in brackets. In case you are not familiar with these
% commands see the JSS style FAQ for details.
%
% Cleaning up \textsc{Bib}{\TeX} files is a somewhat tedious task -- especially
% when acquiring the entries automatically from mixed online sources. However,
% it is important that informations are complete and presented in a consistent
% style to avoid confusions. JSS requires the following format.
% \begin{itemize}
%   \item JSS-specific markup (\verb|\proglang|, \verb|\pkg|, \verb|\code|) should
%     be used in the references.
%   \item Titles should be in title case.
%   \item Journal titles should not be abbreviated and in title case.
%   \item DOIs should be included where available.
%   \item Software should be properly cited as well. For \proglang{R} packages
%     \code{citation("pkgname")} typically provides a good starting point.
% \end{itemize}
% \end{leftbar}
%
% \end{appendix}

%% -----------------------------------------------------------------------------


\end{document}
