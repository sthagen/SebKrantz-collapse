---
title: "Developing with collapse"
subtitle: "Or: How to Code Efficiently in R"
author: "Sebastian Krantz"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true

vignette: >
  %\VignetteIndexEntry{developing with collapse}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{css, echo=FALSE}
pre {
  max-height: 500px;
  overflow-y: auto;
}

pre[class] {
  max-height: 500px;
}
```

```{r, echo=FALSE}
oldopts <- options(width = 100L)
```

```{r, echo = FALSE, message = FALSE, warning=FALSE}
knitr::opts_chunk$set(error = FALSE, message = FALSE, warning = FALSE,
                      comment = "#", tidy = FALSE, cache = FALSE, collapse = TRUE,
                      fig.width = 8, fig.height = 5,
                      out.width = '100%')

library(collapse)
```


## Introduction

*collapse* offers an integrated suite of C/C++-based statistical and data manipulation functions, many low-level tools for memory efficient programming, and a [class-agnostic architecture](https://sebkrantz.github.io/collapse/articles/collapse_object_handling.html) that seamlessly supports vectors, matrices, and data.frame-like objects. These features make it an ideal backend for high-performance statistical packages. This vignette is meant to provide some recommendations for developing with *collapse*. It is complementary to the earlier [blog post on programming with *collapse*](https://sebkrantz.github.io/Rblog/2020/09/13/programming-with-collapse/) which readers are also highly recommended to consult. The vignette adds 3 important points for writing efficient R/*collapse* code.

But before I jump into those let me start with an important warning which is that, while *collapse* is conveniently globally configurable using the `set_collapse()` function, this function should NEVER be called in a package: These are global options that will also affect *collapse*'s behavior outside of your package.

## Point 1: Be Minimalistic Regarding Computations

*collapse* supports different types of R objects (vectors, matrices, data frames + variants) and it can perform grouped operations on them using different types of grouping information (plain vector(s), 'qG'^[Alias for quick-group.] objects, factors, 'GRP' objects, grouped or indexed data frames). Grouping can be sorted or unsorted. A key for very efficient code is to use the minimal required operations/objects to get the job done.

For example, suppose you have the simple task of summing an object `x` by groups using a grouping vector `g`. If this grouping is only needed once, it should be done using the internal grouping of `fsum()` without creating any external grouping objects, e.g., `fsum(x, g)` for aggregation and `fsum(x, g, TRA = "fill")` for expansion. The expansion case is very efficient internally because it uses unsorted grouping. In general, apart from the default sorted aggregation, these functions are pretty smart in converting your vector input `g` into the minimally required grouping information.

In the aggregation case, we can improve performance by also using unsorted grouping, e.g., `fsum(x, qF(g, sort = FALSE))` or `fsum(x, qG(g, sort = FALSE), use.g.names = FALSE)` if the row-names are not needed. If it is known that `g` is a plain vector or the first-appearance order of groups should be kept even if `g` is a factor, use `group(g)` instead of `qG(g, sort = FALSE)`.^[`group()` directly calls a C-based hashing algorithm which works for all types of vectors and lists of vectors/data frames. Missing values are treated as distinct elements.]

If the same operation is needed on multiple vectors/objects, save the factor/'qG' object and reuse it. If not using `group()` to create 'qG', add the argument `na.exclude = FALSE` to `qF()/qG()` to add a class 'na.included' to the object which precludes internal missing value checks in `fsum()` and friends. Always set `use.g.names = FALSE` if not needed, and, if your data has no missing values, set `na.rm = FALSE` in statistical functions like `fsum()` for maximum performance.

Overall, note that factors/'qG' objects are efficient inputs for computations with all statistical/transformation functions except for `fmedian()`, `fnth()`, `fmode()`, `fndistinct()`, and split-apply-combine operations using `BY()/gsplit()`.

For repeated grouped operations involving these latter functions, it makes sense to create 'GRP' objects using `g = GRP(g)`. These objects are more expensive to create but provide more complete information - see `?GRP`, in particular the 'Value' section. If sorting is not needed, set `sort = FALSE`, and if aggregation or the unique groups/group names are not needed set `return.groups = FALSE`.

Only in rare cases are grouped/indexed data frames created with `fgroup_by()`/`findex_by()` needed in package code. Likewise, functions like `fsummarise()/fmutate()` are essentially wrappers. For example
```{r}
mtcars |>
  fgroup_by(cyl, vs, am) |>
  fsummarise(mpg = fsum(mpg),
             across(c(carb, hp, qsec), fmean))
```

Is the same as (`use = FALSE` abbreviates `use.g.names = FALSE`)

```{r}
g <- GRP(mtcars, c("cyl", "vs", "am"))

add_vars(g$groups,
  get_vars(mtcars, "mpg") |> fsum(g, use = FALSE),
  get_vars(mtcars, c("carb", "hp", "qsec")) |> fmean(g, use = FALSE)
)
```

Nothing prevents you from using these wrappers - they are quite efficient - but if you want to change all inputs programmatically it makes sense to go down one level.

In general, think carefully about how to vectorize expressions (using *collapse* or auxiliary packages) in a minimalistic and memory efficient way. You will find that you can craft very parsimonious and efficient code to solve complicated problems.

For example, after merging multiple spatial datasets, I had some of the same map features (businesses) from multiple sources, and, unwilling to match features individually across data sources, I decided to keep the richest source on covering each feature type and location. After creating a feature `importance` indicator comparable across sources, the deduplication expression ended up being a single line of the form: `fsubset(data, source == fmode(source, list(location, type), importance, "fill"))`, i.e., keep features from the importance-weighted mode (i.e., most frequent) source by location and type.

If an effective *collapse* solution is not apparent, other packages may offer efficient solutions. Check out the [*fastverse*](https://fastverse.github.io/fastverse/) and its [suggested packages list](https://fastverse.github.io/fastverse/#suggested-extensions). For example if you want to efficiently replace multiple items in a vector, `kit::vswitch()/nswitch()` can be pretty magical. Also functions like `data.table::set()/rowid()` etc. are great, e.g., [recent issue](https://github.com/SebKrantz/collapse/issues/627): what is the *collapse* equivalent to a grouped `dplyr::slice_head(n)`? It would be `fsubset(data, data.table::rowid(id1, id2, ...) <= n)`.


## Point 2: Think About Memory and Optimize

R programs are inefficient for 2 principal reasons: (1) operations are not vectorized; (2) too many intermediate objects/copies are created. *collapse*'s vectorized statistical functions address (1) effectively, but it also provides many [efficient programming functions](https://sebkrantz.github.io/collapse/reference/efficient-programming.html) to deal with (2).

One source of inefficiency in R code is the widespread use of logical vectors. For example

```{r}
x <- abs(round(rnorm(1e6)))
x[x == 0] <- NA
```

where `x == 0` creates a logical vector of 1 million elements just to indicate to R which elements of `x` are `0`. In *collapse*, `setv(x, 0, NA)` is the efficient equivalent. This also works if we don't want to replace with `NA` but with another vector y, e.g.,

```{r}
y <- rnorm(1e6)
setv(x, NA, y) # Replaces missing x with y
```
is much better than
```{r}
x[is.na(x)] <- y[is.na(x)]
```
`setv()` is quite versatile and also works with indices and logical vectors instead of elements to search for, and it can invert the query setting `invert = TRUE`.

In more complex workflows, we may want to save the logical vector, e.g. `xmiss <- is.na(x)` and use it repeatedly. One aspect to note here is that logical vectors are inefficient for subsetting compared to indices:

```{r}
xNA <- na_insert(x, prop = 0.4)
xmiss <- is.na(xNA)
ind <- which(xmiss)
bench::mark(x[xmiss], x[ind])
```

Thus, indices are always preferable. With *collapse*, they can be created directly using `whichNA(xNA)` in this case, or using `whichv(x, 0)` for `which(x == 0)` or any other number. Also here there exist an `invert = TRUE` argument covering the `!=` case. For convenience, infix operators `x %==% 0` and `x %!=% 0` wrap `whichv(x, 0)` and `whichv(x, 0, invert = TRUE)`, respectively.

Similarly, the function `fmatch()` supports faster matching with associated operators `%iin%` and `%!iin%` which also return indices, e.g., `letters %iin% c("a", "b")` returns `1:2`. This can also be used in subsetting:

```{r}
bench::mark(
  `%in%` = fsubset(wlddev, iso3c %in% c("USA", "DEU", "ITA", "GBR")),
  `%iin%` = fsubset(wlddev, iso3c %iin% c("USA", "DEU", "ITA", "GBR"))
)
```

Likewise, `anyNA(), allNA(), anyv()` and `allv()` help avoid expressions like `any(x == 0)` in favor of `anyv(x, 0)`. Other convenience functions exist such as `na_rm(x)` for the common `x[!is.na(x)]` expression which is extremely inefficient.

Another hint here particularly for data frame subsetting is the `ss()` function, which has an argument `check = FALSE` to avoid checks on indices (small effect with this data size):

```{r}
ind <- wlddev$iso3c %!iin% c("USA", "DEU", "ITA", "GBR")
microbenchmark::microbenchmark(
  withcheck = ss(wlddev, ind),
  nocheck = ss(wlddev, ind, check = FALSE)
)
```

Another common source of inefficiencies is copies produced in statistical operations. For example

```{r}
x <- rnorm(100); y <- rnorm(100); z <- rnorm(100)
res <- x + y + z # Creates 2 copies
```
For this particular case `res <- kit::psum(x, y, z)` offers a direct efficient solution^[In general, also see other packages, in particular *kit* and *data.table* for useful programming functions.]. A more general solution is
```{r}
res <- x + y
res %+=% z
```
*collapse*'s `%+=%`, `%-=%`, `%*=%` and `%/=%` operators are wrappers around the `setop()` function which also works with matrices and data frames.^[*Note* that infix operators do not obey the rules of arithmetic but are always evaluated from left to right.] This function also supports a `rowwise` argument for operations between vectors and matrix/data.frame rows, e.g.

```{r}
m <- qM(mtcars)
setop(m, "*", seq_col(m), rowwise = TRUE)
head(m / qM(mtcars))
```
Some functions like `na_locf()/na_focb()` also have `set = TRUE` arguments to perform operations by reference. There is also the function `setTRA()` for (grouped) transformations by reference, which wraps `TRA(..., set = TRUE)`. Since `TRA` is added as an argument to all [*Fast Statistical Functions*](https://sebkrantz.github.io/collapse/reference/fast-statistical-functions.html), `set = TRUE` can be passed down to modify by reference. For example:

```{r}
fmedian(iris$Sepal.Length, iris$Species, TRA = "fill", set = TRUE)
```
Is the same as `setTRA(iris$Sepal.Length, fmedian(iris$Sepal.Length, iris$Species), "fill", iris$Species)`, replacing the values of the `Sepal.Length` vector with its species median by reference:
```{r}
head(iris)
```
This `set` argument can be invoked anywhere, also inside `fmutate()` calls with/without groups. This can also be done in combination with other transformations (sweeping operations). For example the following turns the columns of the matrix into proportions.

```{r}
fsum(m, TRA = "/", set = TRUE)
fsum(m) # Check
```

In summary, think what is really needed to complete a task and keep things to a minimum in terms of both computations and memory. Let's do a final exercise in this regard and create a hyper-efficient function for univariate linear regression by groups:

```{r}
greg <- function(y, x, g) {
  g <- group(g)
  dmx <- fmean(x, g, TRA = "-", na.rm = FALSE)
  (fsum(y, g, dmx, use.g.names = FALSE, na.rm = FALSE) %/=%
   fsum(dmx, g, dmx, use.g.names = FALSE, na.rm = FALSE))
}

# Test
y <- rnorm(1e7)
x <- rnorm(1e7)
g <- sample.int(1e6, 1e7, TRUE)

microbenchmark::microbenchmark(greg(y, x, g), group(g))
```

The expression computed by `greg()` amounts to `sum(y * (x - mean(x)))/sum((x - mean(x))^2)` for each group, which is equivalent to `cov(x, y)/var(x)`, but very efficient, requiring exactly one full copy of `x` to create a group-demeaned vector, `dmx`, and then using the `w` (weights) argument to `fsum()` to sum the products (`y * dmx` and `dmx * dmx`) on the fly, including a division by reference avoiding an additional copy. One cannot do much better coding a grouped regression directly in C, and ostensibly the grouping step already accounts for half of the computing time.


## Point 3: Internally Favor Primitive R Objects and Functions

This partly reiterates Point 1 but now with a focus on internal data representation rather than grouping and computations. The point could also be bluntly stated as: 'vectors, matrices and lists are good, data frames and complex objects are bad'.

Many frameworks seem to imply the opposite, e.g., the *tidyverse* encourages you to cast your data as a tidy tibble, and *data.table* offers you a more efficient data frame. But these objects are internally complex, and, in the case of *data.table*, only efficient because of the internal C-level algorithms for large-data manipulation. You should always take a step back to ask yourself: For the statistical software I am writing, do I need this complexity? Complex objects require complex methods to manipulate them, thus, when using them, you incur the cost of everything that goes on in these methods. Vectors, matrices, and lists are much more efficient in R and *collapse* provides you with many options to manipulate them directly.

It may surprise you to hear that, internally, *collapse* does not use data frame-like objects at all. Instead, such objects are cast to lists using `unclass(data)`, `class(data) <- NULL`, or `attributes(data) <- NULL`. This is advisable if you want to write fast package code for data frame-like objects. Of course there are limits, e.g., for advanced manipulations such as `join()` or `pivot()`, but many tasks can be accomplished using simple objects. If you don't want to internally convert data frames to lists, use functions `.subset()` and `.subset2()` to efficiently extract columns and `attr()` to extract/set attributes. With matrices use `dimnames()` directly instead of `rownames()` and `colnames()` which wrap it.

The benchmark below illustrates that everything you do on the data.frame is more expensive than on the equivalent list.

```{r}
l <- unclass(mtcars)
nam <- names(mtcars)
microbenchmark::microbenchmark(names(mtcars), attr(mtcars, "names"), names(l),
               names(mtcars) <- nam, attr(mtcars, "names") <- nam, names(l) <- nam,
               mtcars[["mpg"]], .subset2(mtcars, "mpg"), l[["mpg"]],
               mtcars[3:8], .subset(mtcars, 3:8), l[3:8],
               ncol(mtcars), length(mtcars), length(unclass(mtcars)), length(l),
               nrow(mtcars), length(.subset2(mtcars, 1L)), length(l[[1L]]))

```


By means of further illustration, let's recreate the `pwnobs()` function in *collapse* which counts pairwise missing values. The list method is written in R. A basic implementation would be:^[By Point 2 this implementation is not ideal because I am creating two logical vectors for each iteration of the inner loop, but I currently don't see any way to write this more efficiently.]

```{r}
pwnobs_list <- function(X) {
    dg <- fnobs(X)
    n <- ncol(X)
    nr <- nrow(X)
    N.mat <- diag(dg)
    for (i in 1:(n - 1L)) {
        miss <- is.na(X[[i]])
        for (j in (i + 1L):n) N.mat[i, j] <- N.mat[j, i] <- nr - sum(miss | is.na(X[[j]]))
    }
    rownames(N.mat) <- names(dg)
    colnames(N.mat) <- names(dg)
    N.mat
}

mtcNA <- na_insert(mtcars, prop = 0.2)
pwnobs_list(mtcNA)
```

Now with the above tips we can optimize this as follows:
```{r}
pwnobs_list_opt <- function(X) {
    dg <- fnobs.data.frame(X)
    class(X) <- NULL
    n <- length(X)
    nr <- length(X[[1L]])
    N.mat <- diag(dg)
    for (i in 1:(n - 1L)) {
        miss <- is.na(X[[i]])
        for (j in (i + 1L):n) N.mat[i, j] <- N.mat[j, i] <- nr - sum(miss | is.na(X[[j]]))
    }
    dimnames(N.mat) <- list(names(dg), names(dg))
    N.mat
}

identical(pwnobs_list(mtcNA), pwnobs_list_opt(mtcNA))

microbenchmark::microbenchmark(pwnobs_list(mtcNA), pwnobs_list_opt(mtcNA))
```

As the benchmark shows, the optimized function is 6x faster on this (small) dataset, and we have changed nothing to the loops doing the computation. With larger data the difference would of course become less stark, but you never know what's going on in methods you have not written and how they scale to larger data. My advice is: avoid them, use simple objects and take full control over your code. This also makes your code more robust and you can create class-agnostic code.

<!--
As another illustration let's say we want to aggregate an input-output table:

```{r}
IOT <- matrix(1, 260, 260)
nam <- as.vector(t(outer(LETTERS, 1:10, paste, sep = ".")))
dimnames(IOT) <- list(nam, nam)
IOT[1:11, 1:11]

IOT_DF <- qDF(IOT, row.names.col = "country_sector")
head(IOT_DF, 3)
```

We can aggregate this as a matrix or data.frame using *collapse* and `data.table::transpose()`:

```{r}

agg_IOT_DF <- function(IOT_DF) {
  IOT_DF |>
    fmutate(country = substr(country_sector, 1, 1)) |>
    fgroup_by(country) |>
    num_vars() |> fsum() |>
    transpose(keep.names = "country_sector", make.names = "country") |>
    fmutate(country = substr(country_sector, 1, 1)) |>
    fgroup_by(country) |>
    num_vars() |> fsum() |>
    transpose(keep.names = "country", make.names = "country")
}

agg_IOT <- function(IOT) {
  f <- qF(substr(dimnames(IOT)[[1L]], 1, 1), na.exclude = FALSE)
  IOT |> fsum(f) |> t() |> fsum(f) |> t()
}

identical(qM(agg_IOT_DF(IOT_DF), 1), agg_IOT(IOT))
microbenchmark::microbenchmark(agg_IOT_DF(IOT_DF), agg_IOT(IOT))
```
-->

When doing this, also avoid `as.data.frame()` and friends to coerce/recreate data frame-like objects. It is quite easy to construct a *data.frame* from a list:

```{r}
attr(l, "row.names") <- .set_row_names(length(l[[1L]]))
class(l) <- "data.frame"
head(l, 2)
```

You can also use *collapse* functions `qDF()`, `qDT()` and `qTBL()` to efficiently convert/create *data.frame*'s, *data.table*'s, and *tibble*'s:

```{r}
library(data.table)
library(tibble)
microbenchmark::microbenchmark(qDT(mtcars), as.data.table(mtcars),
                               qTBL(mtcars), as_tibble(mtcars))

l <- unclass(mtcars)
microbenchmark::microbenchmark(qDF(l), as.data.frame(l), as.data.table(l), as_tibble(l))

```

*collapse* also provides functions like `setattrib()`, `copyMostAttrib()`, etc., to efficiently attach attributes again. So another efficient workflow for general data frame-like objects is to save the attributes `ax <- attributes(data)`, manipulate it as a list `l <- unclass(data)`, modify `ax$names` and `ax$row.names` as needed and then use `setattrib(l, ax)` to turn it into a *data.frame* again before returning. This way your code will also work with *tibble* etc.

## Conclusion

*collapse* can become a game-changer for your statistical software development in R, enabling you to write programs that effectively run like C while accomplishing complex statistical/data tasks with few lines of code. This however requires taking a closer look at the package, in particular the [documentation](https://sebkrantz.github.io/collapse/reference/collapse-documentation.html), and following the three points given in this vignette.

