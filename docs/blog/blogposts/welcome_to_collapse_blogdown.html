<!DOCTYPE html>
<html lang="en-us">
  <head><link rel="shortcut icon" href="https://sebkrantz.github.io/collapse/favicon.ico" type="image/x-icon">
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.72.0" />


<title>Welcome to collapse: Advanced and Fast Data Transformation in R - A Hugo website</title>
<meta property="og:title" content="Welcome to collapse: Advanced and Fast Data Transformation in R">


 <!-- <link href='/favicon.ico' rel='icon' type='image/x-icon'/> -->












<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="css/fonts.css" media="all">
<link rel="stylesheet" href="css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">


<div id="navbar" class="nav">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Documentation</a>
</li>
<li>
  <a href="../articles/index.html">Vignettes</a>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
<li>
  <a href="https://sebkrantz.github.io/Rblog/" target="_blank">Blog</a>
</li>
</ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://twitter.com/collapse">
    <span class="fa fa-twitter"></span>

  </a>
</li>
<li>
  <a href="https://github.com/SebKrantz/collapse" target="_blank">
    <span class="fa fa-github"></span>

  </a>
</li>
      </ul>
</div>

  <!--      <nav class="nav">
          <a href="/" class="nav-logo">
            <img src="/images/logo.png"
                 width="50"
                 height="50"
                 alt="Logo">
          </a>

          <ul class="nav-links">

            <li><a href="/about/">About</a></li>

            <li><a href="https://github.com/rstudio/blogdown">GitHub</a></li>

            <li><a href="https://twitter.com/rstudio">Twitter</a></li>

          </ul>
        </nav>
-->
      </header>


<main class="content" role="main">

  <article class="article">

   <!-- <span class="article-duration">28 min read</span> -->


    <h1 class="article-title">Welcome to collapse: Advanced and Fast Data Transformation in R</h1>


    <span class="article-date">2020-06-08</span>


    <div class="article-content">



<p><img src='post/collapselogosmall.png' width="150px" align="right"  style="vertical-align:middle;margin:30px 30px" /></p>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p><em>collapse</em> was released on CRAN end of March. The current version 1.2.1 performs well on all operating systems, passing &gt; 5600 unit tests covering all core functionality, thus a good point to start introducing the package to a larger audience. <em>collapse</em> has 2 main aims:</p>
<ol style="list-style-type: decimal">
<li><p>To facilitate complex data transformation and exploration tasks in R.
<em>(In particular grouped and weighted statistical computations, advanced aggregation of multi-type data, advanced transformations of time-series and panel-data, and the manipulation of lists)</em></p></li>
<li><p>To help make R code fast, flexible, parsimonious and programmer friendly. <em>(Provide order of magnitude performance improvements via extensive use of C++ and highly optimized R code, broad object orientation, and infrastructure for grouped programming)</em></p></li>
</ol>
<p>It can be installed in R using:</p>
<pre class="r"><code>install.packages(&#39;collapse&#39;)

# See Documentation
help(&#39;collapse-documentation&#39;)</code></pre>
<p>With this post I want to formally introduce <em>collapse</em> and share some of the motivation and history of it. Then I will provide a basic demonstration of important features, and end with a small benchmark comparing <em>collapse</em> to <em>dplyr</em> and <em>data.table</em>. The key features and functions of the package are summarized in the figure below.</p>
<div class="figure">
<img src="post/collapseheader.png" width = "100%" alt="" />
<p class="caption"><em>collapse</em> Core Functions</p>
</div>
<p>I start with the motivation (you can skip this if you like).</p>
</div>
<div id="some-motivation-and-history" class="section level1">
<h1>Some Motivation and History</h1>
<p>I work as an applied economist in the broader fields of macroeconomics, trade and development. As such I frequently encounter survey data such as household or enterprise surveys, longitudinal / panel data such as cross-country, trade, geospatial panels or panel-surveys, and multivariate macroeconomic data such as groups of monthly or quarterly economic time-series.</p>
<p>On this data I usually require various grouped computations, such as aggregating multi-type data (including groupwise-weighted statistics for survey data or trade-share weighted stuff), grouped scaling or centering panel data in preparation for some econometric techniques programmed by hand (sometimes with weights on a panel-survey), and various time-series stuff such as computing sequences of lags / leads, (iterated) differences, quasi-differences and growth rates / log-differences on large panel data (i.e. trade or geospatial panels) and large groups of macroeconomic time-series. I also found myself often faced with recursive extraction and unlisting / row-binding problems, particularly for multivariate time-series stuff like impulse responses from a VAR which I like to turn into a data.frame and visualize with <em>ggplot2</em>.</p>
<!-- On this data I usually compute various grouped computations, such as aggregating multi-type survey data which requires groupwise-weighted statistics (i.e. weighted mean, or weighted mode for categorical data) or trade-share weighted stuff etc., grouped scaling or centering panel data (sometimes centering on multiple groups and sometimes with weights on a panel-survey) in preparation for some advanced econometric techniques I program by hand, and various time-series stuff such as computing sequences of lags / leags, (iterated) differences, quasi-differnces (for serial correlation issues) and growth rates / log-differences on large panel data (i.e. geospatial panels) and large groups of macroeconomic time-series (i.e. in preparation for dynamic factor analysis and related techniques). -->
<!-- With these demands I soon realized that the *tidyverse*, while in many ways very useful and instructive, was not really serving me well. Why? Because it is focussed on data.frames thus requiring conversion and often loss of attributes, it does not really support weighted or grouped and weighted computations, it is very limited in terms of transformations and time-computations, and the non-standard evaluation and piped syntax does not make for easy or efficient programming. With *data.table* I found more possibilities to do grouped and weighted computations, rolling statistics and some quite fast aggregations, but I soon also got dissatisfied with the requirement of converting everything to data.table first and the limited support for (fast) grouped transformation, time-computations and programming. I also discovered that the methods and classes offered by the *plm* package are very useful to manipulate, explore and program with panel data, but they are also not very broad in scope and quite slow as data grow large. Finally, I noticed that many statistical methods, particularly multivariate time-series stuff like Vector-Autoregressions etc., provide results in terms of (sometimes nested) lists of statistics matrices which I often wanted to turn into a data.frame for *ggplot*-ing or further analysis, and spent quite a lot of time doing that.  -->
<p>With these demands I soon realized that neither the <em>tidyverse</em> nor <em>data.table</em> were servicing me very well. The reasons being the requirement of converting to data.frame/data.table/tibble, limited support for weighted computations, efficient time-series and panel-data transformations, and, (in my opinion) limited utility for programming because of non-standard evaluation and R overhead. I also found some object-oriented packages quite useful, especially <em>plm</em> for panel-data manipulation, but soon bumped into serious constraints regarding performance and scope of implementation. The same holds true for useful model-extraction packages like <em>broom</em>, <em>insight</em>, <em>parameters</em> etc. which don’t support multivariate time-series stuff and lack some flexibility in the format and type of information extracted.</p>
<!-- Finally, I noticed that many statistical methods, particularly multivariate time-series stuff like Vector-Autoregressions etc., provide important results in terms of (sometimes nested) lists of statistics matrices which I often wanted to turn into a data.frame for *ggplot*-ing or further analysis, and spent quite a lot of time doing that.  -->
<!-- , while in many ways very useful and instructive, was not really serving me well. Why? Because it is focussed on data.frames thus requiring conversion and often loss of attributes, it does not really support weighted or grouped and weighted computations, it is very limited in terms of transformations and time-computations, and the non-standard evaluation and piped syntax does not make for easy or efficient programming. With *data.table* I found more possibilities to do grouped and weighted computations, rolling statistics and some quite fast aggregations, but I soon also got dissatisfied with the requirement of converting everything to data.table first and the limited support for (fast) grouped transformation, time-computations and programming. I also discovered that the methods and classes offered by the *plm* package are very useful to manipulate, explore and program with panel data, but they are also not very broad in scope and quite slow as data grow large. Finally, I noticed that many statistical methods, particularly multivariate time-series stuff like Vector-Autoregressions etc., provide results in terms of (sometimes nested) lists of statistics matrices which I often wanted to turn into a data.frame for *ggplot*-ing or further analysis, and spent quite a lot of time doing that.  -->
<p>Thus I came to the conclusion that if I wanted to take things further in R, I had to create my own data manipulation package with the following properties:</p>
<ul>
<li>A broad object-oriented approach with generic functions supporting vectors, matrices, data.frames and lists. The approach should preserve object classes and avoid unnecessary conversions and loss of attributes (such as attributes of time-series, data.frame’s or columns in a data.frame).</li>
</ul>
<!-- and various useful classes such as time-series, panel-series / panel data.frames and grouped data.frames -->
<ul>
<li><p>Support very fast grouped and weighted computations (aggregations, transformations, time-series and panel-data stuff and general replacing and sweeping out of statistics) on all those objects.</p></li>
<li><p>Provide facilities for efficient (grouped) programming in R using standard evaluation (so that I could write panel-estimators etc. with a few lines of expressive code).</p></li>
<li><p>Provide various useful functions that would facilitate frequent complex tasks, such as (weighted) aggregation of multi-type data, processing lists (i.e. recursive search and extraction from list-like objects and recursive unlisting to data.frame), and some essential tools such as panel-data statistics and autocorrelation functions, support for variable labels etc..</p></li>
</ul>
<!-- even packages like *data.table* have some R overhead slowing it down quite a bit on smaller datasets, and that  -->
<p>These objectives synthesized into an ambitious project over time. I started about 2 years ago with the functions <code>collap</code> and <code>qsu</code> for multi-type data aggregation and panel-summary statistics. I soon discovered that my R code was somewhat complex and did not deliver the performance I had hoped for. For a while I thought about using <em>data.table</em> backend, but then I encountered <em>Rcpp</em> and studied <em>data.table</em> source a bit. I discovered that developing grouped statistical functions with <em>Rcpp</em> and combining them with <em>data.table</em>’s C-functions for fast radix-order based grouping could produce serially compiled code that was faster than <em>data.table</em> itself run with two threads on my laptop. Thus I decided to develop sets of grouped and weighted functions and transformation operators, which allowed a much faster and more flexible package than using a backend. I also discovered that I could optimize critical base R operations such as selecting columns or subsetting data.frames using efficient primitives and some C/C++ code. Thus performance became an objective and I aimed to</p>
<!-- and service them with grouping objects based on *data.table*'s grouping C-code. This -->
<ul>
<li>Develop the fastest serially compiled package for data manipulation in R, use C/C++ code whenever sensible and thoroughly optimize the R code.</li>
</ul>
<!-- (Possibly parallelism will be added later but it is not top of the agenda). -->
<p>Then I thought that this could also become a quite useful add-on package for users of <em>dplyr</em> / <em>tidyverse</em>, <em>data.table</em> and <em>plm</em>. Thus I decided to integrate <em>collapse</em> into these programming environments:</p>
<!-- To service them I chose to integrate *collapse* with these packages and allow for seamless harmony and coexistence, adding an additional objective. -->
<ul>
<li>Offer seamless integration with <em>dplyr</em>, <em>data.table</em> and <em>plm</em> through support of relevant classes (<em>grouped_df</em>, <em>data.table</em>, <em>pseries</em>, <em>pdata.frame</em>), some extra methods and added non-standard evaluation features.</li>
</ul>
<p>I also thought that the <em>collapse</em> API for grouped programming in R, with fast C++ based grouped functions serviced with grouping objects efficiently created and accessed in R, could be useful for other developers, both in R and C++. Thus I opted to make this all available to the user and document it. I will shortly write a post covering fast grouped programming with <em>collapse</em>.</p>
<ul>
<li>Make <em>collapse</em> maximally programmer / developer friendly, through making available fast grouping mechanisms, core methods and some utilities facilitating efficient grouped programming in R.</li>
</ul>
<p>Regarding documentation, I do not really like the default R format of function-Rd pages supplemented by vignettes. I got inspired by <em>Mathematica</em> which has a beautiful structured documentation starting from a central overview page. Thus my final development objective:</p>
<ul>
<li>Create a structured, horizonatally and vertically integrated package documentation which can be called from the R console.</li>
</ul>
<p><em>collapse</em> also has some quite extensive vignettes supplementing the built-in documentation and discussing the integration with <em>dplyr</em> and <em>plm</em>.
<!-- That's it up to this point in time. *collapse* 1.2.1 is ready to be used.  --></p>
</div>
<div id="demonstration" class="section level1">
<h1>Demonstration</h1>
<p>I will start by briefly demonstrating the <em>Fast Statistical Functions</em>, which are a central feature of <em>collapse</em>. Currently there are 13 of them (<code>fmean</code>, <code>fmedian</code>, <code>fmode</code>, <code>fsum</code>, <code>fprod</code>, <code>fsd</code>, <code>fvar</code>, <code>fmin</code>, <code>fmax</code>, <code>ffirst</code>, <code>flast</code>, <code>fnobs</code> and <code>fndistinct</code>), they are all S3 generic and support fast grouped computations on vectors, matrices, data.frame’s, lists and grouped tibble’s (class <em>grouped_df</em>). Calling these functions on different objects yields simple column-wise statistical computations:</p>
<pre class="r"><code>library(collapse)
v &lt;- iris$Sepal.Length
d &lt;- num_vars(iris)    # Saving numeric variables
g &lt;- iris$Species

# Simple statistics
fmean(v)              # vector
## [1] 5.843333
fmean(qM(d))          # matrix (qM is a faster as.matrix)
## Sepal.Length  Sepal.Width Petal.Length  Petal.Width
##     5.843333     3.057333     3.758000     1.199333
fmean(d)              # data.frame
## Sepal.Length  Sepal.Width Petal.Length  Petal.Width
##     5.843333     3.057333     3.758000     1.199333

# Preserving data structure
fmean(qM(d), drop = FALSE)     # still a matrix
##      Sepal.Length Sepal.Width Petal.Length Petal.Width
## [1,]     5.843333    3.057333        3.758    1.199333
fmean(d, drop = FALSE)         # still a data.frame
##   Sepal.Length Sepal.Width Petal.Length Petal.Width
## 1     5.843333    3.057333        3.758    1.199333</code></pre>
<p>The functions <code>fmean</code>, <code>fsum</code>, <code>fprod</code>, <code>fmode</code>, <code>fvar</code> and <code>fsd</code> additionally support weights<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
<pre class="r"><code># Weighted statistics, similarly for vectors and matrices ...
w &lt;- abs(rnorm(fnrow(iris)))
fmean(d, w = w)
## Sepal.Length  Sepal.Width Petal.Length  Petal.Width
##     5.852019     3.062309     3.778201     1.214366</code></pre>
<p>The second argument of these functions is called <code>g</code> and supports vectors or lists of grouping variables for grouped computations. For functions supporting weights, <code>w</code> is the third argument. I note that all further examples generalize to computations on vectors and matrices.
<!-- it does not matter anymore on which type of object we are working.   --></p>
<pre class="r"><code># Grouped statistics
fmean(d, g)
##            Sepal.Length Sepal.Width Petal.Length Petal.Width
## setosa            5.006       3.428        1.462       0.246
## versicolor        5.936       2.770        4.260       1.326
## virginica         6.588       2.974        5.552       2.026

# Groupwise-weighted statistics
fmean(d, g, w)
##            Sepal.Length Sepal.Width Petal.Length Petal.Width
## setosa         5.070850    3.436514     1.497392   0.2461531
## versicolor     5.986126    2.802493     4.320558   1.3455506
## virginica      6.467102    2.982225     5.408647   2.0151582</code></pre>
<p>Grouping becomes more efficient when factors or grouping objects are passed to <code>g</code>. Factors can efficiently be created using the function <code>qF</code>, and grouping objects are efficiently created with the function <code>GRP</code><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> (more in a future post about programming). In addition, all functions support transformations through the <code>TRA</code> argument.</p>
<pre class="r"><code># Simple Transformations
head(fvar(v, TRA = &quot;replace&quot;))  # replacing values with the overall variance
## [1] 0.6856935 0.6856935 0.6856935 0.6856935 0.6856935 0.6856935
head(fsd(v, TRA = &quot;/&quot;))         # dividing by the overall standard-deviation (scaling)
## [1] 6.158928 5.917402 5.675875 5.555112 6.038165 6.521218

# Grouped transformations
head(fvar(v, g, TRA = &quot;replace&quot;))  # replacing values with the group variance
## [1] 0.124249 0.124249 0.124249 0.124249 0.124249 0.124249
head(fsd(v, g, TRA = &quot;/&quot;))         # grouped scaling
## [1] 14.46851 13.90112 13.33372 13.05003 14.18481 15.31960
head(fmin(v, g, TRA = &quot;-&quot;))        # setting the minimum value in each species to 0
## [1] 0.8 0.6 0.4 0.3 0.7 1.1
head(fsum(v, g, TRA = &quot;/&quot;))        # dividing by the sum (proportions)
## [1] 0.02037555 0.01957651 0.01877747 0.01837795 0.01997603 0.02157411
head(fmedian(v, g, TRA = &quot;-&quot;))     # de-median
## [1]  0.1 -0.1 -0.3 -0.4  0.0  0.4
head(ffirst(v, g, TRA = &quot;%%&quot;))     # taking modulus of first group-value, etc ...
## [1] 0.0 4.9 4.7 4.6 5.0 0.3

# Grouped and weighted transformations
head(fsd(v, g, w, &quot;/&quot;), 3)         # Weighted scaling
## [1] 15.23123 14.63393 14.03663
head(fmode(d, g, w, &quot;replace&quot;), 3) # replace with weighted statistical mode
##   Sepal.Length Sepal.Width Petal.Length Petal.Width
## 1            5         3.5          1.5         0.2
## 2            5         3.5          1.5         0.2
## 3            5         3.5          1.5         0.2</code></pre>
<p>Currently there are 10 different replacing or sweeping operations supported by <code>TRA</code>, see <code>?TRA</code>. <code>TRA</code> can also be called directly as a function which performs simple and grouped replacing and sweeping operations:</p>
<pre class="r"><code>head(TRA(v, fmedian(v), &quot;-&quot;))                        # Same as fmedian(v, TRA = &quot;-&quot;)
## [1] -0.7 -0.9 -1.1 -1.2 -0.8 -0.4
head(TRA(d, BY(d, g, quantile, 0.05), &quot;replace&quot;, g)) # Replace values with 5% percentile by species
##   Sepal.Length Sepal.Width Petal.Length Petal.Width
## 1          4.4           3          1.2         0.1
## 2          4.4           3          1.2         0.1
## 3          4.4           3          1.2         0.1
## 4          4.4           3          1.2         0.1
## 5          4.4           3          1.2         0.1
## 6          4.4           3          1.2         0.1</code></pre>
<p>I have just used the function <code>BY</code>, which is also generic for split-apply-combine computing with user-supplied functions. Another useful function is <code>dapply</code> (data-apply), which supports quite efficient column- and row-wise operations on matrices and data.frames using user-supplied functions.</p>
<!-- I note that simple row-wise operations on data.frames like row-sums are best performed through efficient matrix conversion i.e. `rowSums(qM(d))` is better than `dapply(d, sum, MARGIN = 1)`.    -->
<p>Some common panel-data transformations like between- and within-transformations (averaging and centering using the mean) are implemented slightly more memory efficient in the functions <code>fbetween</code> and <code>fwithin</code>. The function <code>fscale</code> also exists for fast (grouped, weighted) scaling and centering (standardizing) and mean-preserving scaling. These functions provide further options for data harmonization, such as centering on the overall data mean or scaling to the within-group standard deviation<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> (as shown below), as well as scaling / centering to arbitrary supplied means and standard deviations.</p>
<pre class="r"><code>oldpar &lt;- par(mfrow = c(1,3))
plot(get_vars(d, 1:2), col = g, main = &quot;Raw Data&quot;)
plot(fwithin(get_vars(d, 1:2), g, mean = &quot;overall.mean&quot;), col = g,
     main = &quot;Centered on Overall Mean&quot;)
plot(fscale(get_vars(d, 1:2), g, mean = &quot;overall.mean&quot;, sd = &quot;within.sd&quot;), col = g,
     main = &quot;Harmonized Mean and Variance&quot;)</code></pre>
<p><img src="post/2020-06-08-welcome-to-collapse_files/figure-html/unnamed-chunk-7-1.png" width="100%" /></p>
<pre class="r"><code>par(oldpar)</code></pre>
<p>The function <code>get_vars</code> is 2x faster than <code>[.data.frame</code>, attribute-preserving, and also supports column selection using functions or regular expressions. It’s replacement version <code>get_vars&lt;-</code> is 6x faster than <code>[&lt;-.data.frame</code>. Apart from <code>fbetween</code> and <code>fwithin</code>, the functions <code>fhdbetween</code> and <code>fhdwithin</code> can average or center data on multiple groups, and they can also project out continuous variables alongside (i.e. they provide fitted values or residuals from regression problems which may or may not involve one or more factors).</p>
<p>For the manipulation of time-series and panel-series, <em>collapse</em> offers the functions <code>flag</code>, <code>fdiff</code> and <code>fgrowth</code>.</p>
<pre class="r"><code>head3 &lt;- function(x) head(x, 3L)
head3(flag(EuStockMarkets, -1:1))      # A sequence of lags and leads
##       F1.DAX     DAX  L1.DAX F1.SMI    SMI L1.SMI F1.CAC    CAC L1.CAC F1.FTSE   FTSE L1.FTSE
## [1,] 1613.63 1628.75      NA 1688.5 1678.1     NA 1750.5 1772.8     NA  2460.2 2443.6      NA
## [2,] 1606.51 1613.63 1628.75 1678.6 1688.5 1678.1 1718.0 1750.5 1772.8  2448.2 2460.2  2443.6
## [3,] 1621.04 1606.51 1613.63 1684.1 1678.6 1688.5 1708.1 1718.0 1750.5  2470.4 2448.2  2460.2
head3(fdiff(EuStockMarkets, 0:1, 1:2)) # First and second difference of each variable
##          DAX D1.DAX D2.DAX    SMI D1.SMI D2.SMI    CAC D1.CAC D2.CAC   FTSE D1.FTSE D2.FTSE
## [1,] 1628.75     NA     NA 1678.1     NA     NA 1772.8     NA     NA 2443.6      NA      NA
## [2,] 1613.63 -15.12     NA 1688.5   10.4     NA 1750.5  -22.3     NA 2460.2    16.6      NA
## [3,] 1606.51  -7.12      8 1678.6   -9.9  -20.3 1718.0  -32.5  -10.2 2448.2   -12.0   -28.6</code></pre>
<p>I note that all attributes of the time-series matrix <code>EuStockMarkets</code> were preserved, the use of <code>head</code> just suppresses the print method.
<!-- At this point I will  -->
<!-- ```{r, eval=FALSE} -->
<!-- library(vars) -->
<!-- library(ggplot2) -->
<!-- library(data.table) # for melt function --></p>
<!-- frequency(EuStockMarkets) -->
<!-- VARselect(EuStockMarkets, type = "both", season = 260) -->
<!-- varmod <- VAR(EuStockMarkets, p = 7, type = "both", season = 260) -->
<!-- serial.test(varmod) -->
<!-- irf <- irf(varmod) -->
<!-- str(irf) -->
<!-- irfdata <- unlist2d(list_elem(irf), idcols = c("bound", "series"), row.names = "time", -->
<!--                     id.factor = TRUE, DT = TRUE) -->
<!-- head(irfdata) -->
<!-- melt(irfdata, 1:3) %>% ggplot(aes(x = time, y = value, colour = series, shape = bound)) + -->
<!--   geom_line() + facet_wrap("variable") -->
<!-- ``` -->
<p>To take things a bit further, let’s consider some multilevel / panel data:</p>
<pre class="r"><code># World Bank World Development Data - supplied with collapse
head3(wlddev)
##       country iso3c       date year decade     region     income  OECD PCGDP LIFEEX GINI       ODA
## 1 Afghanistan   AFG 1961-01-01 1960   1960 South Asia Low income FALSE    NA 32.292   NA 114440000
## 2 Afghanistan   AFG 1962-01-01 1961   1960 South Asia Low income FALSE    NA 32.742   NA 233350000
## 3 Afghanistan   AFG 1963-01-01 1962   1960 South Asia Low income FALSE    NA 33.185   NA 114880000</code></pre>
<p>All variables in this data have labels stored in a ‘label’ attribute (the default if you import with <em>haven</em>). Variable labels can be accessed and set using <code>vlabels</code> and <code>vlabels&lt;-</code>, and viewed together with names and classes using <code>namlab</code>. In general variable labels and other attributes will be preserved in when working with <em>collapse</em>. <em>collapse</em> provides some of the fastest and most advanced summary statistics:</p>
<pre class="r"><code># Distinct value count
fndistinct(wlddev)
## country   iso3c    date    year  decade  region  income    OECD   PCGDP  LIFEEX    GINI     ODA
##     216     216      59      59       7       7       4       2    8995   10048     363    7564
# Use descr(wlddev) for a detailed description of each variable

# Checking for within-country variation
varying(wlddev, ~ iso3c)
## country    date    year  decade  region  income    OECD   PCGDP  LIFEEX    GINI     ODA
##   FALSE    TRUE    TRUE    TRUE   FALSE   FALSE   FALSE    TRUE    TRUE    TRUE    TRUE

# Panel-data statistics: Summarize GDP and GINI overall, between and within countries
qsu(wlddev, pid = PCGDP + GINI ~ iso3c,
    vlabels = TRUE, higher = TRUE)
## , , PCGDP: GDP per capita (constant 2010 US$)
##
##            N/T      Mean        SD        Min        Max  Skew   Kurt
## Overall   8995  11563.65  18348.41     131.65  191586.64  3.11  16.96
## Between    203  12488.86  19628.37      255.4  141165.08  3.21  17.25
## Within   44.31  11563.65   6334.95  -30529.09   75348.07   0.7  17.05
##
## , , GINI: GINI index (World Bank estimate)
##
##           N/T   Mean    SD    Min    Max  Skew  Kurt
## Overall  1356   39.4  9.68   16.2   65.8  0.46  2.29
## Between   161  39.58  8.37  23.37  61.71  0.52  2.67
## Within   8.42   39.4  3.04  23.96   54.8  0.14  5.78

# All of that by region, only summarizing GDP, not computing higher moments
aperm(qsu(wlddev, by = ~ region,
                 pid = PCGDP ~ iso3c))[,, 1:2]
## , , East Asia &amp; Pacific
##
##                  N/T         Mean           SD          Min          Max
## Overall   1391.00000  10337.04635  14094.83383    131.96337  72183.30331
## Between     34.00000  10337.04635  12576.61999    410.20040  40046.40886
## Within      40.91176  11563.65287   6363.40871 -11892.07023  50475.98727
##
## , , Europe &amp; Central Asia
##
##                  N/T         Mean           SD          Min          Max
## Overall   2084.00000  25664.80639  26181.67097    367.04926 191586.63965
## Between     56.00000  25664.80639  24008.10062    788.02286 141165.08286
## Within      37.21429  11563.65287  10444.66367 -30529.09280  75348.06699

# Within-country correlations with p-value and observation count
pwcor(fwithin(get_vars(wlddev, 9:12), wlddev$iso3c),
      N = TRUE, P = TRUE)
##               PCGDP        LIFEEX         GINI          ODA
## PCGDP    1   (8995)   .30* (8398) -.03  (1342) -.01  (6852)
## LIFEEX  .30* (8398)   1   (11068) -.15* (1353)  .14* (7746)
## GINI   -.03  (1342)  -.15* (1353)   1   (1356)  -.02  (951)
## ODA    -.01  (6852)   .14* (7746)  -.02  (951)   1   (8336)

# Panel-data ACF: Efficient grouped standardizing and computing covariance with panel-lags
# (same as stats version, I might add psAcf to also give the forecast version)
psacf(wlddev, ~ iso3c, ~ year, cols = 9:12)</code></pre>
<p><img src="post/2020-06-08-welcome-to-collapse_files/figure-html/unnamed-chunk-10-1.png" width="100%" /></p>
<p>For fast grouped statistics we can keep programming in standard evaluation as before, or we can use piped expressions.</p>
<pre class="r"><code>head3(fmean(get_vars(wlddev, 9:12),
            get_vars(wlddev, c(&quot;region&quot;, &quot;income&quot;))))
##                                             PCGDP   LIFEEX     GINI       ODA
## East Asia &amp; Pacific.High income         26042.280 73.22799 32.80000 177672692
## East Asia &amp; Pacific.Lower middle income  1621.178 58.83796 36.21081 503484782
## East Asia &amp; Pacific.Upper middle income  3432.559 66.41750 42.29524 242080501

`%&gt;%` &lt;- magrittr::`%&gt;%`
wlddev %&gt;% fgroup_by(region, income) %&gt;%
  fselect(PCGDP:ODA) %&gt;% fmean %&gt;% head3
##                region              income     PCGDP   LIFEEX     GINI       ODA
## 1 East Asia &amp; Pacific         High income 26042.280 73.22799 32.80000 177672692
## 2 East Asia &amp; Pacific Lower middle income  1621.178 58.83796 36.21081 503484782
## 3 East Asia &amp; Pacific Upper middle income  3432.559 66.41750 42.29524 242080501</code></pre>
<p>I note that the default is <code>na.rm = TRUE</code> for all <em>collapse</em> functions<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> I also note that you can also use <code>dplyr::group_by</code> and <code>dplyr::select</code>, but <code>fgroup_by</code> and <code>fselect</code> are significantly faster (see benchmark). We can do a weighted aggregation using the variable <code>ODA</code> as weights using:</p>
<pre class="r"><code># Weighted group mean: Weighted by ODA
wlddev %&gt;% fgroup_by(region, income) %&gt;%
  fselect(PCGDP:ODA) %&gt;% fmean(ODA) %&gt;% head3
##                region              income      sum.ODA    PCGDP   LIFEEX     GINI
## 1 East Asia &amp; Pacific         High income  64672860000 2332.191 64.61962       NA
## 2 East Asia &amp; Pacific Lower middle income 346397530000 1411.414 62.52892 36.23540
## 3 East Asia &amp; Pacific Upper middle income 106273340000 1706.960 68.76040 44.63886</code></pre>
<p>Note that in this case by default (<code>keep.w = TRUE</code>) the sum of the weights is also computed and saved. <em>collapse</em> also has its own data aggregation command called <code>collap</code>. It is in many ways more flexible, for example you can apply multiple functions to each column and efficiently reshape the result.</p>
<pre class="r"><code>collap(wlddev, by = ~ region + income,
       FUN = list(fmean, fsd), cols = 9:12) %&gt;% head3
##                region              income fmean.PCGDP  fsd.PCGDP fmean.LIFEEX fsd.LIFEEX fmean.GINI
## 1 East Asia &amp; Pacific         High income   26042.280 14593.6195     73.22799   6.363266   32.80000
## 2 East Asia &amp; Pacific Lower middle income    1621.178   908.9503     58.83796   9.383965   36.21081
## 3 East Asia &amp; Pacific Upper middle income    3432.559  2418.0954     66.41750   6.451834   42.29524
##   fsd.GINI fmean.ODA   fsd.ODA
## 1 1.222020 177672692 337269272
## 2 4.830619 503484782 693802678
## 3 3.640621 242080501 604615277
collap(wlddev, by = ~ region + income,
       FUN = list(fmean, fsd), cols = 9:12, return = &quot;long&quot;) %&gt;% head3
##   Function              region              income     PCGDP   LIFEEX     GINI       ODA
## 1    fmean East Asia &amp; Pacific         High income 26042.280 73.22799 32.80000 177672692
## 2    fmean East Asia &amp; Pacific Lower middle income  1621.178 58.83796 36.21081 503484782
## 3    fmean East Asia &amp; Pacific Upper middle income  3432.559 66.41750 42.29524 242080501</code></pre>
<p><code>collap</code> also supports flexible multi-type aggregation, here applying the mean to numeric and statistical mode to categorical data. The default (<code>keep.col.order = TRUE</code>) ensures that the data remains in the same order, and, when working with <em>Fast Statistical Functions</em>, all column attributes are preserved.</p>
<pre class="r"><code># Applying the mean to numeric and the mode to categorical data (first 2 arguments are &#39;by&#39; and &#39;FUN&#39;)
collap(wlddev, ~ iso3c + decade, fmean, catFUN = fmode) %&gt;% head3
##   country iso3c       date   year decade                     region      income  OECD PCGDP   LIFEEX
## 1   Aruba   ABW 1961-01-01 1962.5   1960 Latin America &amp; Caribbean  High income FALSE    NA 66.58583
## 2   Aruba   ABW 1967-01-01 1970.0   1970 Latin America &amp; Caribbean  High income FALSE    NA 69.14178
## 3   Aruba   ABW 1976-01-01 1980.0   1980 Latin America &amp; Caribbean  High income FALSE    NA 72.17600
##   GINI      ODA
## 1   NA       NA
## 2   NA       NA
## 3   NA 33630000

# Same as a piped call.. without column reordering
wlddev %&gt;% fgroup_by(iso3c, decade) %&gt;% {
  add_vars(fmode(cat_vars(.)), fmean(get_vars(., 9:12))) # cat_vars selects non-numeric (categorical) columns
} %&gt;% head3
##   iso3c decade country       date                     region      income  OECD iso3c decade PCGDP
## 1   ABW   1960   Aruba 1961-01-01 Latin America &amp; Caribbean  High income FALSE   ABW   1960    NA
## 2   ABW   1970   Aruba 1967-01-01 Latin America &amp; Caribbean  High income FALSE   ABW   1970    NA
## 3   ABW   1980   Aruba 1976-01-01 Latin America &amp; Caribbean  High income FALSE   ABW   1980    NA
##     LIFEEX GINI      ODA
## 1 66.58583   NA       NA
## 2 69.14178   NA       NA
## 3 72.17600   NA 33630000

# Adding weights: weighted mean and weighted mode (catFUN is 3rd argument)
collap(wlddev, ~ iso3c + decade, fmean, fmode, w = ~ ODA) %&gt;% head3
##   country iso3c       date year decade                     region      income  OECD PCGDP LIFEEX
## 1   Aruba   ABW 1966-01-01   NA   1960 Latin America &amp; Caribbean  High income    NA    NA     NA
## 2   Aruba   ABW 1975-01-01   NA   1970 Latin America &amp; Caribbean  High income    NA    NA     NA
## 3   Aruba   ABW 1986-01-01 1985   1980 Latin America &amp; Caribbean  High income FALSE    NA 73.181
##   GINI      ODA
## 1   NA       NA
## 2   NA       NA
## 3   NA 33630000

# Fully custom aggregation (also possible with weights)
collap(wlddev, ~ iso3c + decade,
            custom = list(fmean = 9:12,
                          fmax = 9:10,
                          flast = cat_vars(wlddev, &quot;indices&quot;),
                          fmode = &quot;GINI&quot;)) %&gt;% head3
##   flast.country iso3c flast.iso3c flast.date decade               flast.region flast.income
## 1         Aruba   ABW         ABW 1966-01-01   1960 Latin America &amp; Caribbean   High income
## 2         Aruba   ABW         ABW 1975-01-01   1970 Latin America &amp; Caribbean   High income
## 3         Aruba   ABW         ABW 1986-01-01   1980 Latin America &amp; Caribbean   High income
##   flast.OECD fmean.PCGDP fmax.PCGDP fmean.LIFEEX fmax.LIFEEX fmean.GINI fmode.GINI fmean.ODA
## 1      FALSE          NA         NA     66.58583      67.435         NA         NA        NA
## 2      FALSE          NA         NA     69.14178      70.519         NA         NA        NA
## 3      FALSE          NA         NA     72.17600      73.181         NA         NA  33630000</code></pre>
<p>When aggregating with multiple functions, you can parallelize over them (internally done with <code>parallel::mclapply</code>).</p>
<p>Time computations on panel-data are also performed fast and simple.</p>
<pre class="r"><code># Panel Lag and lead of PCGDP and LIFEEX
L(wlddev, -1:1, PCGDP + LIFEEX ~ iso3c, ~year) %&gt;% head3
##   iso3c year F1.PCGDP PCGDP L1.PCGDP F1.LIFEEX LIFEEX L1.LIFEEX
## 1   AFG 1960       NA    NA       NA    32.742 32.292        NA
## 2   AFG 1961       NA    NA       NA    33.185 32.742    32.292
## 3   AFG 1962       NA    NA       NA    33.624 33.185    32.742

# Equivalent piped call
wlddev %&gt;% fgroup_by(iso3c) %&gt;% fselect(iso3c, year, PCGDP, LIFEEX) %&gt;%
  flag(-1:1, year) %&gt;% head3
##   iso3c year F1.PCGDP PCGDP L1.PCGDP F1.LIFEEX LIFEEX L1.LIFEEX
## 1   AFG 1960       NA    NA       NA    32.742 32.292        NA
## 2   AFG 1961       NA    NA       NA    33.185 32.742    32.292
## 3   AFG 1962       NA    NA       NA    33.624 33.185    32.742

# Growth rates in percentage terms: 1 and 10-year
G(wlddev, c(1, 10), 1, ~ iso3c, ~year, cols = 9:12) %&gt;% head3 # or use Dlog, or G(..., logdiff = TRUE) for percentages
##   iso3c year G1.PCGDP L10G1.PCGDP G1.LIFEEX L10G1.LIFEEX G1.GINI L10G1.GINI    G1.ODA L10G1.ODA
## 1   AFG 1960       NA          NA        NA           NA      NA         NA        NA        NA
## 2   AFG 1961       NA          NA  1.393534           NA      NA         NA 103.90598        NA
## 3   AFG 1962       NA          NA  1.353002           NA      NA         NA -50.76923        NA</code></pre>
<p>Equivalently we can can compute leaded and suitably iterated (log-) differences and growth rates, as well as quasi-(log-)differences of the form <span class="math inline">\(x_t - \rho x_{t-1}\)</span>. The operators <code>L</code> and <code>G</code> are shorthand’s for the functions <code>flag</code> and <code>fgrowth</code> allowing formula input. Similar operators exist for <code>fwithin</code>, <code>fscale</code>, etc.</p>
</div>
<div id="benchmark" class="section level1">
<h1>Benchmark</h1>
<p>For benchmarking we use some product-level trade data from the UN Comtrade database, processed by .</p>
<pre class="r"><code>library(tradestatistics)
# US HS4-level trade from 2000 to 2018
us_trade &lt;- ots_create_tidy_data(years = 2000:2018,
                                 reporters = &quot;usa&quot;,
                                 table = &quot;yrpc&quot;)</code></pre>
<p>Downloading US product-level trade (HS4) from 2000 to 2018 gives about 2.6 million observations:</p>
<pre class="r"><code>fdim(us_trade)
## [1] 2569787      16
head3(us_trade)
##    year reporter_iso                                                   reporter_fullname_english
## 1: 2017          usa USA, Puerto Rico and US Virgin Islands (excludes Virgin Islands until 1981)
## 2: 2018          usa USA, Puerto Rico and US Virgin Islands (excludes Virgin Islands until 1981)
## 3: 2017          usa USA, Puerto Rico and US Virgin Islands (excludes Virgin Islands until 1981)
##    partner_iso partner_fullname_english section_code section_color section_shortname_english
## 1:         afg              Afghanistan           01       #74c0e2           Animal Products
## 2:         afg              Afghanistan           01       #74c0e2           Animal Products
## 3:         afg              Afghanistan           01       #74c0e2           Animal Products
##         section_fullname_english group_code group_fullname_english product_code
## 1: Live Animals; Animal Products         01          Animals; live         0101
## 2: Live Animals; Animal Products         01          Animals; live         0101
## 3: Live Animals; Animal Products         01          Animals; live         0102
##    product_shortname_english               product_fullname_english export_value_usd
## 1:                    Horses Horses, asses, mules and hinnies; live             3005
## 2:                    Horses Horses, asses, mules and hinnies; live            15699
## 3:                    Bovine                   Bovine animals; live             3436
##    import_value_usd
## 1:               NA
## 2:               NA
## 3:               NA

# 19 years, 221 trading partners, 1222 products, unbalanced panel with product-time gaps...
qDF(rbind(class = vclasses(us_trade),
          Ndistinct = fndistinct(us_trade)))
##              year reporter_iso reporter_fullname_english partner_iso partner_fullname_english
## class     integer    character                 character   character                character
## Ndistinct      19            1                         1         221                      221
##           section_code section_color section_shortname_english section_fullname_english group_code
## class        character     character                 character                character  character
## Ndistinct           22            22                        22                       22         97
##           group_fullname_english product_code product_shortname_english product_fullname_english
## class                  character    character                 character                character
## Ndistinct                     97         1222                      1217                     1222
##           export_value_usd import_value_usd
## class              numeric          numeric
## Ndistinct          1081492           684781

# Summarizing data between and within partner-product pairs
qsu(us_trade, pid = export_value_usd + import_value_usd ~ partner_iso + product_code)
## , , export_value_usd
##
##               N/T         Mean           SD              Min             Max
## Overall  2,450301  11,054800.6   157,295999                1  2.83030606e+10
## Between    205513  7,268011.31   118,709845                1  1.66436161e+10
## Within      11.92  11,054800.6  68,344396.5  -1.01599067e+10  1.67185229e+10
##
## , , import_value_usd
##
##               N/T         Mean          SD              Min             Max
## Overall  1,248201  31,421502.4  505,644905                1  8.51970855e+10
## Between    130114  16,250758.2  328,538895                1  4.36545695e+10
## Within       9.59  31,421502.4  212,076350  -3.32316111e+10  4.15739375e+10</code></pre>
<p>It would also be interesting to summarize the trade flows for each partner, but that would be too large to print to the console. We can however get the <code>qsu</code> output as a list of matrices:</p>
<pre class="r"><code># Doing all of that by partner - variance of flows between and within traded products for each partner
l &lt;- qsu(us_trade, by = export_value_usd + import_value_usd ~ partner_iso,
                   pid = ~ partner_iso + product_code, array = FALSE)
str(l)
## List of 2
##  $ export_value_usd:List of 3
##   ..$ Overall: &#39;qsu&#39; num [1:221, 1:5] 7250 12427 6692 5941 4017 ...
##   .. ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. .. ..$ : chr [1:221] &quot;afg&quot; &quot;ago&quot; &quot;aia&quot; &quot;alb&quot; ...
##   .. .. ..$ : chr [1:5] &quot;N&quot; &quot;Mean&quot; &quot;SD&quot; &quot;Min&quot; ...
##   ..$ Between: &#39;qsu&#39; num [1:221, 1:5] 901 1151 872 903 695 ...
##   .. ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. .. ..$ : chr [1:221] &quot;afg&quot; &quot;ago&quot; &quot;aia&quot; &quot;alb&quot; ...
##   .. .. ..$ : chr [1:5] &quot;N&quot; &quot;Mean&quot; &quot;SD&quot; &quot;Min&quot; ...
##   ..$ Within : &#39;qsu&#39; num [1:221, 1:5] 8.05 10.8 7.67 6.58 5.78 ...
##   .. ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. .. ..$ : chr [1:221] &quot;afg&quot; &quot;ago&quot; &quot;aia&quot; &quot;alb&quot; ...
##   .. .. ..$ : chr [1:5] &quot;N&quot; &quot;Mean&quot; &quot;SD&quot; &quot;Min&quot; ...
##  $ import_value_usd:List of 3
##   ..$ Overall: &#39;qsu&#39; num [1:221, 1:5] 1157 1547 361 1512 685 ...
##   .. ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. .. ..$ : chr [1:221] &quot;afg&quot; &quot;ago&quot; &quot;aia&quot; &quot;alb&quot; ...
##   .. .. ..$ : chr [1:5] &quot;N&quot; &quot;Mean&quot; &quot;SD&quot; &quot;Min&quot; ...
##   ..$ Between: &#39;qsu&#39; num [1:221, 1:5] 312 532 167 347 235 ...
##   .. ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. .. ..$ : chr [1:221] &quot;afg&quot; &quot;ago&quot; &quot;aia&quot; &quot;alb&quot; ...
##   .. .. ..$ : chr [1:5] &quot;N&quot; &quot;Mean&quot; &quot;SD&quot; &quot;Min&quot; ...
##   ..$ Within : &#39;qsu&#39; num [1:221, 1:5] 3.71 2.91 2.16 4.36 2.91 ...
##   .. ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. .. ..$ : chr [1:221] &quot;afg&quot; &quot;ago&quot; &quot;aia&quot; &quot;alb&quot; ...
##   .. .. ..$ : chr [1:5] &quot;N&quot; &quot;Mean&quot; &quot;SD&quot; &quot;Min&quot; ...</code></pre>
<p>Now with the function <code>unlist2d</code>, we can efficiently turn this into a tidy data.table:</p>
<pre class="r"><code>unlist2d(l, idcols = c(&quot;Variable&quot;, &quot;Trans&quot;),
         row.names = &quot;Partner&quot;, DT = TRUE)
##               Variable   Trans Partner            N        Mean         SD        Min        Max
##    1: export_value_usd Overall     afg  7250.000000  2170073.97 21176449.3         56 1115125722
##    2: export_value_usd Overall     ago 12427.000000  2188174.59 17158413.8          1  687323408
##    3: export_value_usd Overall     aia  6692.000000   125729.27   586862.2       2503   17698445
##    4: export_value_usd Overall     alb  5941.000000   236526.91  1725825.4          1   57390901
##    5: export_value_usd Overall     and  4017.000000    68714.93   352731.2          1    9250532
##   ---
## 1322: import_value_usd  Within     wsm     2.741333 31421502.37   394042.1   30120744   40746421
## 1323: import_value_usd  Within     yem     3.166144 31421502.37 14233554.6 -149711729  262267521
## 1324: import_value_usd  Within     zaf    14.681780 31421502.37 33241070.2 -988968293 1824699995
## 1325: import_value_usd  Within     zmb     3.691244 31421502.37  3566277.6   13737510  134309327
## 1326: import_value_usd  Within     zwe     4.986607 31421502.37  3706416.2    3562060  163570062</code></pre>
<p>If <code>l</code> were some statistical object we could first pull out relevant elements using <code>get_elem</code>, possibly process those elements using <code>rapply2d</code> and then apply <code>unlist2d</code> to get the data.frame (or data.table with <code>DT = TRUE</code>). These are the main <em>collapse</em> list-processing functions.</p>
<p>Now on to the benchmark. It is run on a Windows 8.1 laptop with a 2x 2.2 GHZ Intel i5 processor, 8GB DDR3 RAM and a Samsung 850 EVO SSD hard drive.</p>
<pre class="r"><code>library(microbenchmark)
library(dplyr)
library(data.table)
setDTthreads(2)     # Default setting for this machine

# Grouping (data.table:::forderv here does not compute the unique groups yet)
microbenchmark(collapse = fgroup_by(us_trade, partner_iso, group_code, year),
               data.table = data.table:::forderv(us_trade, c(&quot;partner_iso&quot;, &quot;group_code&quot;, &quot;year&quot;), retGrp = TRUE),
               dplyr = group_by(us_trade, partner_iso, group_code, year), times = 10)
## Unit: milliseconds
##        expr      min       lq     mean   median       uq       max neval
##    collapse 114.3742 119.3588 123.1506 124.0317 127.1472  132.1068    10
##  data.table 139.5368 143.2915 145.9903 145.6225 148.4252  157.3158    10
##       dplyr 936.2200 943.7585 988.6936 959.1369 985.3425 1205.1616    10

# Sum
microbenchmark(collapse = collap(us_trade, export_value_usd + import_value_usd ~ partner_iso + group_code + year, fsum),
               data.table = us_trade[, list(export_value_usd = sum(export_value_usd, na.rm = TRUE),
                                            import_value_usd = sum(import_value_usd, na.rm = TRUE)),
                                      by = c(&quot;partner_iso&quot;, &quot;group_code&quot;, &quot;year&quot;)],
               dplyr = group_by(us_trade, partner_iso, group_code, year) %&gt;%
                       dplyr::select(export_value_usd, import_value_usd) %&gt;% summarise_all(sum, na.rm = TRUE), times = 10)
## Unit: milliseconds
##        expr      min        lq      mean    median        uq       max neval
##    collapse 140.3874  145.3902  159.9978  154.1423  174.4445  204.8755    10
##  data.table 214.4288  222.1440  252.1965  225.6026  230.7494  493.6184    10
##       dplyr 967.3574 1021.0027 1063.7001 1063.4654 1111.7382 1152.3200    10

# Mean
microbenchmark(collapse = collap(us_trade, export_value_usd + import_value_usd ~ partner_iso + group_code + year, fmean),
               data.table = us_trade[, list(export_value_usd = mean(export_value_usd, na.rm = TRUE),
                                            import_value_usd = mean(import_value_usd, na.rm = TRUE)),
                                     by = c(&quot;partner_iso&quot;, &quot;group_code&quot;, &quot;year&quot;)],
               dplyr = group_by(us_trade, partner_iso, group_code, year) %&gt;%
                 dplyr::select(export_value_usd, import_value_usd) %&gt;% summarise_all(mean, na.rm = TRUE), times = 10)
## Unit: milliseconds
##        expr       min        lq      mean    median        uq       max neval
##    collapse  141.0590  144.9145  153.7647  153.0974  160.3520  170.4828    10
##  data.table  225.8643  230.6303  238.8578  235.0673  250.2772  258.4061    10
##       dplyr 7827.3275 7974.5759 8100.9639 8037.2186 8244.1311 8633.0780    10

# Replace with group-sum
microbenchmark(collapse = fgroup_by(us_trade, partner_iso, group_code, year) %&gt;%
                 fselect(export_value_usd, import_value_usd) %&gt;% fsum(TRA = &quot;replace_fill&quot;),
               data.table = us_trade[, `:=`(export_value_usd2 = sum(export_value_usd, na.rm = TRUE),
                                             import_value_usd2 = sum(import_value_usd, na.rm = TRUE)),
                                      by = c(&quot;partner_iso&quot;, &quot;group_code&quot;, &quot;year&quot;)],
               dplyr = group_by(us_trade, partner_iso, group_code, year) %&gt;%
                 dplyr::select(export_value_usd, import_value_usd) %&gt;% mutate_all(sum, na.rm = TRUE), times = 10)
## Unit: milliseconds
##        expr       min        lq      mean    median        uq       max neval
##    collapse  171.0843  188.0078  203.1915  199.6288  222.3015  233.6697    10
##  data.table  944.8107  949.1773  986.1224  962.1044  987.0137 1143.9542    10
##       dplyr 1931.3653 1990.2562 2133.1637 2138.9475 2196.5392 2416.0891    10

# Centering, partner-product
microbenchmark(collapse = fgroup_by(us_trade, partner_iso, product_code) %&gt;%
                 fselect(export_value_usd, import_value_usd) %&gt;% fwithin,
               data.table = us_trade[, `:=`(export_value_usd2 = export_value_usd - mean(export_value_usd, na.rm = TRUE),
                                            import_value_usd2 = import_value_usd - mean(import_value_usd, na.rm = TRUE)),
                                     by = c(&quot;partner_iso&quot;, &quot;group_code&quot;, &quot;year&quot;)],
               dplyr = group_by(us_trade, partner_iso, group_code, year) %&gt;%
                 dplyr::select(export_value_usd, import_value_usd) %&gt;% mutate_all(function(x) x - mean(x, na.rm = TRUE)), times = 10)
## Unit: milliseconds
##        expr      min        lq      mean    median        uq        max neval
##    collapse  119.166  122.0332  131.1729  130.1562  131.5284   154.9765    10
##  data.table 4557.702 4933.2145 5995.6182 6386.9886 6465.8386  7070.7664    10
##       dplyr 7168.567 9391.0167 9457.3973 9582.8031 9798.0586 10529.7284    10

# Lag
# Much better to sort data for dplyr
setorder(us_trade, partner_iso, product_code, year)
# We have an additional problem here: There are time-gaps within some partner-product pairs
tryCatch(L(us_trade, 1, export_value_usd + import_value_usd ~ partner_iso + product_code, ~ year),
         error = function(e) e)
## &lt;Rcpp::exception in L.data.frame(us_trade, 1, export_value_usd + import_value_usd ~     partner_iso + product_code, ~year): Gaps in timevar within one or more groups&gt;
# The solution is that we create a unique id for each continuous partner-product sequence
settransform(us_trade, id = unattrib(seqid(year + unattrib(finteraction(partner_iso, product_code)) * 20L)))
# Notes: Normally id = seqid(year) would be enough on sorted data, but here we also have very different start and end dates...thus is some cases the next group starts the year after the former ends, checked with: table(with(us_trade, ffirst(fdiff(year), list(partner_iso, product_code))) == 1). unattrib removes all attributes, giving a plain integer id (just done for the benchmark here, otherwise collapse would have an advantage).

fndistinct(us_trade$id)
## [1] 423884
# Another nice comparison... fmode is similarly fast for categorical aggregation...
microbenchmark(fndistinct(us_trade$id), n_distinct(us_trade$id))
## Unit: milliseconds
##                     expr       min        lq      mean    median        uq       max neval
##  fndistinct(us_trade$id)  28.04757  28.50877  29.62054  28.89656  29.53983  40.52868   100
##  n_distinct(us_trade$id) 121.75249 123.37014 125.34740 124.79768 126.48271 143.47405   100

# Here we go now:
microbenchmark(collapse = L(us_trade, 1, export_value_usd + import_value_usd ~ id),
               collapse_ordered = L(us_trade, 1, export_value_usd + import_value_usd ~ id, ~ year),
               data.table = us_trade[, shift(.SD), keyby = id,
                                     .SDcols = c(&quot;export_value_usd&quot;,&quot;import_value_usd&quot;)],
               data.table_ordered = us_trade[order(year), shift(.SD), keyby = id,
                                     .SDcols = c(&quot;export_value_usd&quot;,&quot;import_value_usd&quot;)],
               dplyr = group_by(us_trade, id) %&gt;% dplyr::select(export_value_usd, import_value_usd) %&gt;%
                       mutate_all(lag), times = 10)
## Unit: milliseconds
##                expr        min         lq       mean     median        uq        max neval
##            collapse   40.18016   55.46370   61.40067   58.77062   69.6883   91.42503    10
##    collapse_ordered   64.75592   77.86312   87.07693   85.86389  101.9975  103.84456    10
##          data.table 7164.71322 7232.25507 7528.33433 7380.25251 7774.5524 8411.57531    10
##  data.table_ordered 7387.64349 7399.37177 7581.10671 7511.55961 7784.9224 7985.29167    10
##               dplyr 1245.86429 1292.29102 1355.59735 1374.94773 1393.6036 1437.96641    10

# Note: you can do ordered lags using mutate_all(lag, order_by = &quot;year&quot;) for dplyr, but at computation times in excess of 90 seconds..</code></pre>
<p>The benchmark shows that <em>collapse</em> is consistently very fast for the operations it provides (this also pertains to weighted computations which are only slightly slower than unweighted ones). But of course <em>collapse</em> cannot do a lot of things you can do with <em>dplyr</em> or <em>data.table</em> and vice-versa. It is and remains an advanced package, but I think it lives up to the high standards set forth by these packages. I am also highly indebted to <em>data.table</em> for inspiration and some vital bits of C-code. Feel free to get in touch for any suggestions or comments about <em>collapse</em>. I hope you will find it useful.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><code>fvar</code> and <code>fsd</code> compute frequency weights, the most common form of weighted sample variance. I am working on weights for <code>fmedian</code>, but still dealing with performance issues.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Grouping objects are better for programming and for multiple grouping variables. This is demonstrated in the blog post on programming with <em>collapse</em>.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>The within-group standard deviation is the standard deviation computed on the group-centered data.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Missing values are efficiently skipped at C++ level with hardly any computational cost. This also pertains to missing values occurring in the weight vector. If <code>na.rm = FALSE</code>, execution will stop when a missing value is encountered, and <code>NA</code> is returned. This also speeds up computations compared to base R, particularly if some columns or some groups have missing values and others not. The fast functions also avoid <code>NaN</code>’s being created from computations involving <code>NA</code> values, and functions like <code>fsum</code> are well behaved (i.e. <code>fsum(NA)</code> gives <code>NA</code>, not <code>0</code> like <code>sum(NA, na.rm = TRUE)</code>, similarly for <code>fmin</code> and <code>fmax</code>).<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

    </div>
  </article>




</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>




<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>




<script src="js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>



  </body>
</html>

