---
title: "Welcome to collapse: Advanced and Fast Data Transformation in R"
author: "Sebastian Krantz"
date: '2020-06-08'
slug: welcome-to-collapse
categories: ["R"]
tags: ["collapse", "advanced", "fast", "transformation", "manipulation", "time-series", "multilevel", "panel", "weighted"]
---

<img src='collapse_logo_small.png' width="150px" align="right" />


```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, fig.width = 8, fig.height = 5, out.width = '100%')

oldopts <- options(width = 101L)

library(data.table)
```

# Introduction

*collapse* was released on CRAN end of March. The current version 1.2.1 performs well on all operating systems, passing > 5600 unit tests covering all core functionality, thus a good point to start introducing the package to a larger audience. *collapse* has 2 main aims: 

1. To facilitate complex data transformation and exploration tasks in R. 
*(In particular grouped and weighted statistical computations, advanced aggregation of multi-type data, advanced transformations of time-series and panel-data, and the manipulation of lists)*

2. To help make R code fast, flexible, parsimonious and programmer friendly. *(Provide order of magnitude performance improvements via extensive use of C++ and highly optimized R code, broad object orientation, and infrastructure for grouped programming)*

It can be installed in R using:

```{r, eval=FALSE}
install.packages('collapse')

# See Documentation
help('collapse-documentation')
```

With this post I want to formally introduce *collapse* and share some of the motivation and history of it. Then I will provide a basic demonstration of important features, and end with a small benchmark comparing *collapse* to *dplyr* and *data.table*. The key features and functions of the package are summarized in the figure below. 

![*collapse* Core Functions](collapse header.png)

I start with the motivation (you can skip this if you like).

# Some Motivation and History 

I work as an applied economist in the broader fields of macroeconomics, trade and development. As such I frequently encounter survey data such as household or enterprise surveys, longitudinal / panel data such as cross-country, trade, geospatial panels or panel-surveys, and multivariate macroeconomic data such as groups of monthly or quarterly economic time-series. 


On this data I usually require various grouped computations, such as aggregating multi-type data (including groupwise-weighted statistics for survey data or trade-share weighted stuff), grouped scaling or centering panel data in preparation for some econometric techniques programmed by hand (sometimes with weights on a panel-survey), and various time-series stuff such as computing sequences of lags / leads, (iterated) differences, quasi-differences and growth rates / log-differences on large panel data (i.e. trade or geospatial panels) and large groups of macroeconomic time-series. I also found myself often faced with recursive extraction and unlisting / row-binding problems, particularly for multivariate time-series stuff like impulse responses from a VAR which I like to turn into a data.frame and visualize with *ggplot2*. 

<!-- On this data I usually compute various grouped computations, such as aggregating multi-type survey data which requires groupwise-weighted statistics (i.e. weighted mean, or weighted mode for categorical data) or trade-share weighted stuff etc., grouped scaling or centering panel data (sometimes centering on multiple groups and sometimes with weights on a panel-survey) in preparation for some advanced econometric techniques I program by hand, and various time-series stuff such as computing sequences of lags / leags, (iterated) differences, quasi-differnces (for serial correlation issues) and growth rates / log-differences on large panel data (i.e. geospatial panels) and large groups of macroeconomic time-series (i.e. in preparation for dynamic factor analysis and related techniques). -->

<!-- With these demands I soon realized that the *tidyverse*, while in many ways very useful and instructive, was not really serving me well. Why? Because it is focussed on data.frames thus requiring conversion and often loss of attributes, it does not really support weighted or grouped and weighted computations, it is very limited in terms of transformations and time-computations, and the non-standard evaluation and piped syntax does not make for easy or efficient programming. With *data.table* I found more possibilities to do grouped and weighted computations, rolling statistics and some quite fast aggregations, but I soon also got dissatisfied with the requirement of converting everything to data.table first and the limited support for (fast) grouped transformation, time-computations and programming. I also discovered that the methods and classes offered by the *plm* package are very useful to manipulate, explore and program with panel data, but they are also not very broad in scope and quite slow as data grow large. Finally, I noticed that many statistical methods, particularly multivariate time-series stuff like Vector-Autoregressions etc., provide results in terms of (sometimes nested) lists of statistics matrices which I often wanted to turn into a data.frame for *ggplot*-ing or further analysis, and spent quite a lot of time doing that.  -->

With these demands I soon realized that neither the *tidyverse* nor *data.table* were servicing me very well. The reasons being the requirement of converting to data.frame/data.table/tibble, limited support for weighted computations, efficient time-series and panel-data transformations, and, (in my opinion) limited utility for programming because of non-standard evaluation and R overhead. I also found some object-oriented packages quite useful, especially *plm* for panel-data manipulation, but soon bumped into serious constraints regarding performance and scope of implementation. The same holds true for useful model-extraction packages like *broom*, *insight*, *parameters* etc. which don't support multivariate time-series stuff and lack some flexibility in the format and type of information extracted. 

<!-- Finally, I noticed that many statistical methods, particularly multivariate time-series stuff like Vector-Autoregressions etc., provide important results in terms of (sometimes nested) lists of statistics matrices which I often wanted to turn into a data.frame for *ggplot*-ing or further analysis, and spent quite a lot of time doing that.  -->


<!-- , while in many ways very useful and instructive, was not really serving me well. Why? Because it is focussed on data.frames thus requiring conversion and often loss of attributes, it does not really support weighted or grouped and weighted computations, it is very limited in terms of transformations and time-computations, and the non-standard evaluation and piped syntax does not make for easy or efficient programming. With *data.table* I found more possibilities to do grouped and weighted computations, rolling statistics and some quite fast aggregations, but I soon also got dissatisfied with the requirement of converting everything to data.table first and the limited support for (fast) grouped transformation, time-computations and programming. I also discovered that the methods and classes offered by the *plm* package are very useful to manipulate, explore and program with panel data, but they are also not very broad in scope and quite slow as data grow large. Finally, I noticed that many statistical methods, particularly multivariate time-series stuff like Vector-Autoregressions etc., provide results in terms of (sometimes nested) lists of statistics matrices which I often wanted to turn into a data.frame for *ggplot*-ing or further analysis, and spent quite a lot of time doing that.  -->


Thus I came to the conclusion that if I wanted to take things further in R, I had to create my own data manipulation package with the following properties:

* A broad object-oriented approach with generic functions supporting vectors, matrices, data.frames and lists. The approach should preserve object classes and avoid unnecessary conversions and loss of attributes (such as attributes of time-series, data.frame's or columns in a data.frame).

<!-- and various useful classes such as time-series, panel-series / panel data.frames and grouped data.frames -->

* Support very fast grouped and weighted computations (aggregations, transformations, time-series and panel-data stuff and general replacing and sweeping out of statistics) on all those objects. 

* Provide facilities for efficient (grouped) programming in R using standard evaluation (so that I could write panel-estimators etc. with a few lines of expressive code). 

* Provide various useful functions that would facilitate frequent complex tasks, such as (weighted) aggregation of multi-type data, processing lists (i.e. recursive search and extraction from list-like objects and recursive unlisting to data.frame), and some essential tools such as panel-data statistics and autocorrelation functions, support for variable labels etc.. 

<!-- even packages like *data.table* have some R overhead slowing it down quite a bit on smaller datasets, and that  -->

These objectives synthesized into an ambitious project over time. I started about 2 years ago with the functions `collap` and `qsu` for multi-type data aggregation and panel-summary statistics. I soon discovered that my R code was somewhat complex and did not deliver the performance I had hoped for. For a while I thought about using *data.table* backend, but then I encountered *Rcpp* and studied *data.table* source a bit. I discovered that developing grouped statistical functions with *Rcpp* and combining them with *data.table*'s C-functions for fast radix-order based grouping could produce serially compiled code that was faster than *data.table* itself run with two threads on my laptop. Thus I decided to develop sets of grouped and weighted functions and transformation operators, which allowed a much faster and more flexible package than using a backend. I also discovered that I could optimize critical base R operations such as selecting columns or subsetting data.frames using efficient primitives and some C/C++ code. Thus performance became an objective and I aimed to

<!-- and service them with grouping objects based on *data.table*'s grouping C-code. This -->

* Develop the fastest serially compiled package for data manipulation in R, use C/C++ code whenever sensible and thoroughly optimize the R code. 

<!-- (Possibly parallelism will be added later but it is not top of the agenda). -->

Then I thought that this could also become a quite useful add-on package for users of *dplyr* / *tidyverse*, *data.table* and *plm*. Thus I decided to integrate *collapse* into these programming environments:

<!-- To service them I chose to integrate *collapse* with these packages and allow for seamless harmony and coexistence, adding an additional objective. -->

* Offer seamless integration with *dplyr*, *data.table* and *plm* through support of relevant classes (*grouped_df*, *data.table*, *pseries*, *pdata.frame*), some extra methods and added non-standard evaluation features.

I also thought that the *collapse* API for grouped programming in R, with fast C++ based grouped functions serviced with grouping objects efficiently created and accessed in R, could be useful for other developers, both in R and C++. Thus I opted to make this all available to the user and document it. I will shortly write a post covering fast grouped programming with *collapse*.  

* Make *collapse* maximally programmer / developer friendly, through making available fast grouping mechanisms, core methods and some utilities facilitating efficient grouped programming in R.

Regarding documentation, I do not really like the default R format of function-Rd pages supplemented by vignettes. I got inspired by *Mathematica* which has a beautiful structured documentation starting from a central overview page. Thus my final development objective:

* Create a structured, horizonatally and vertically integrated package documentation which can be called from the R console. 

*collapse* also has some quite extensive vignettes supplementing the built-in documentation and discussing the integration with *dplyr* and *plm*. 
<!-- That's it up to this point in time. *collapse* 1.2.1 is ready to be used.  -->

# Demonstration 

I will start by briefly demonstrating the *Fast Statistical Functions*, which are a central feature of *collapse*. Currently there are 13 of them (`fmean`, `fmedian`, `fmode`, `fsum`, `fprod`, `fsd`, `fvar`, `fmin`, `fmax`, `ffirst`, `flast`, `fnobs` and `fndistinct`), they are all S3 generic and support fast grouped computations on vectors, matrices, data.frame's, lists and grouped tibble's (class *grouped_df*). Calling these functions on different objects yields simple column-wise statistical computations:

```{r, message=FALSE}
library(collapse)
v <- iris$Sepal.Length 
d <- num_vars(iris)    # Saving numeric variables
g <- iris$Species

# Simple statistics
fmean(v)              # vector
fmean(qM(d))          # matrix (qM is a faster as.matrix)
fmean(d)              # data.frame

# Preserving data structure
fmean(qM(d), drop = FALSE)     # still a matrix
fmean(d, drop = FALSE)         # still a data.frame
```
The functions `fmean`, `fsum`, `fprod`, `fmode`, `fvar` and `fsd` additionally support weights^[`fvar` and `fsd` compute frequency weights, the most common form of weighted sample variance. I am working on weights for `fmedian`, but still dealing with performance issues.].

```{r}
# Weighted statistics, similarly for vectors and matrices ...
w <- abs(rnorm(fnrow(iris)))
fmean(d, w = w)     
```

The second argument of these functions is called `g` and supports vectors or lists of grouping variables for grouped computations. For functions supporting weights, `w` is the third argument. I note that all further examples generalize to computations on vectors and matrices. 
<!-- it does not matter anymore on which type of object we are working.   -->

```{r}
# Grouped statistics
fmean(d, g) 

# Groupwise-weighted statistics
fmean(d, g, w) 
```

Grouping becomes more efficient when factors or grouping objects are passed to `g`. Factors can efficiently be created using the function `qF`, and grouping objects are efficiently created with the function `GRP`^[Grouping objects are better for programming and for multiple grouping variables. This is demonstrated in the blog post on programming with *collapse*.] (more in a future post about programming). In addition, all functions support transformations through the `TRA` argument. 

```{r}
# Simple Transformations
head(fvar(v, TRA = "replace"))  # replacing values with the overall variance
head(fsd(v, TRA = "/"))         # dividing by the overall standard-deviation (scaling)

# Grouped transformations
head(fvar(v, g, TRA = "replace"))  # replacing values with the group variance
head(fsd(v, g, TRA = "/"))         # grouped scaling
head(fmin(v, g, TRA = "-"))        # setting the minimum value in each species to 0
head(fsum(v, g, TRA = "/"))        # dividing by the sum (proportions)
head(fmedian(v, g, TRA = "-"))     # de-median
head(ffirst(v, g, TRA = "%%"))     # taking modulus of first group-value, etc ...

# Grouped and weighted transformations
head(fsd(v, g, w, "/"), 3)         # Weighted scaling
head(fmode(d, g, w, "replace"), 3) # replace with weighted statistical mode
```

Currently there are 10 different replacing or sweeping operations supported by `TRA`, see `?TRA`. `TRA` can also be called directly as a function which performs simple and grouped replacing and sweeping operations: 

```{r}
head(TRA(v, fmedian(v), "-"))                        # Same as fmedian(v, TRA = "-")
head(TRA(d, BY(d, g, quantile, 0.05), "replace", g)) # Replace values with 5% percentile by species
```

I have just used the function `BY`, which is also generic for split-apply-combine computing with user-supplied functions. Another useful function is `dapply` (data-apply), which supports quite efficient column- and row-wise operations on matrices and data.frames using user-supplied functions. 

<!-- I note that simple row-wise operations on data.frames like row-sums are best performed through efficient matrix conversion i.e. `rowSums(qM(d))` is better than `dapply(d, sum, MARGIN = 1)`.    -->

Some common panel-data transformations like between- and within-transformations (averaging and centering using the mean) are implemented slightly more memory efficient in the functions `fbetween` and `fwithin`. The function `fscale` also exists for fast (grouped, weighted) scaling and centering (standardizing) and mean-preserving scaling. These functions provide further options for data harmonization, such as centering on the overall data mean or scaling to the within-group standard deviation^[The within-group standard deviation is the standard deviation computed on the group-centered data.] (as shown below), as well as scaling / centering to arbitrary supplied means and standard deviations. 

```{r, collapse=FALSE}
oldpar <- par(mfrow = c(1,3))
plot(get_vars(d, 1:2), col = g, main = "Raw Data")                      
plot(fwithin(get_vars(d, 1:2), g, mean = "overall.mean"), col = g, 
     main = "Centered on Overall Mean")
plot(fscale(get_vars(d, 1:2), g, mean = "overall.mean", sd = "within.sd"), col = g,    
     main = "Harmonized Mean and Variance")
par(oldpar)

```

The function `get_vars` is 2x faster than `[.data.frame`, attribute-preserving, and also supports column selection using functions or regular expressions. It's replacement version `get_vars<-` is 6x faster than `[<-.data.frame`. Apart from `fbetween` and `fwithin`, the functions `fhdbetween` and `fhdwithin` can average or center data on multiple groups, and they can also project out continuous variables alongside (i.e. they provide fitted values or residuals from regression problems which may or may not involve one or more factors). 

For the manipulation of time-series and panel-series, *collapse* offers the functions `flag`, `fdiff` and `fgrowth`. 
 
```{r}
head3 <- function(x) head(x, 3L)
head3(flag(EuStockMarkets, -1:1))      # A sequence of lags and leads
head3(fdiff(EuStockMarkets, 0:1, 1:2)) # First and second difference of each variable
```

I note that all attributes of the time-series matrix `EuStockMarkets` were preserved, the use of `head` just suppresses the print method.
<!-- At this point I will  -->
<!-- ```{r, eval=FALSE} -->
<!-- library(vars) -->
<!-- library(ggplot2) -->
<!-- library(data.table) # for melt function -->

<!-- frequency(EuStockMarkets) -->
<!-- VARselect(EuStockMarkets, type = "both", season = 260) -->
<!-- varmod <- VAR(EuStockMarkets, p = 7, type = "both", season = 260) -->
<!-- serial.test(varmod) -->
<!-- irf <- irf(varmod) -->
<!-- str(irf) -->
<!-- irfdata <- unlist2d(list_elem(irf), idcols = c("bound", "series"), row.names = "time", -->
<!--                     id.factor = TRUE, DT = TRUE) -->
<!-- head(irfdata) -->

<!-- melt(irfdata, 1:3) %>% ggplot(aes(x = time, y = value, colour = series, shape = bound)) + -->
<!--   geom_line() + facet_wrap("variable") -->

<!-- ``` -->


To take things a bit further, let's consider some multilevel / panel data: 

```{r}
# World Bank World Development Data - supplied with collapse
head3(wlddev)
```
All variables in this data have labels stored in a 'label' attribute (the default if you import with *haven*). Variable labels can be accessed and set using `vlabels` and `vlabels<-`, and viewed together with names and classes using `namlab`. In general variable labels and other attributes will be preserved in when working with *collapse*. *collapse* provides some of the fastest and most advanced summary statistics:
```{r}
# Distinct value count
fndistinct(wlddev)
# Use descr(wlddev) for a detailed description of each variable

# Checking for within-country variation
varying(wlddev, ~ iso3c)

# Panel-data statistics: Summarize GDP and GINI overall, between and within countries
qsu(wlddev, pid = PCGDP + GINI ~ iso3c, 
    vlabels = TRUE, higher = TRUE)

# All of that by region, only summarizing GDP, not computing higher moments
aperm(qsu(wlddev, by = ~ region,
                 pid = PCGDP ~ iso3c))[,, 1:2]

# Within-country correlations with p-value and observation count
pwcor(fwithin(get_vars(wlddev, 9:12), wlddev$iso3c), 
      N = TRUE, P = TRUE)

# Panel-data ACF: Efficient grouped standardizing and computing covariance with panel-lags
# (same as stats version, I might add psAcf to also give the forecast version)
psacf(wlddev, ~ iso3c, ~ year, cols = 9:12)
```

For fast grouped statistics we can keep programming in standard evaluation as before, or we can use piped expressions. 

```{r, message=FALSE}
head3(fmean(get_vars(wlddev, 9:12), 
            get_vars(wlddev, c("region", "income"))))

`%>%` <- magrittr::`%>%` 
wlddev %>% fgroup_by(region, income) %>% 
  fselect(PCGDP:ODA) %>% fmean %>% head3
```

I note that the default is `na.rm = TRUE` for all *collapse* functions^[Missing values are efficiently skipped at C++ level with hardly any computational cost. This also pertains to missing values occurring in the weight vector. If `na.rm = FALSE`, execution will stop when a missing value is encountered, and `NA` is returned. This also speeds up computations compared to base R, particularly if some columns or some groups have missing values and others not. The fast functions also avoid `NaN`'s being created from computations involving `NA` values, and functions like `fsum` are well behaved (i.e. `fsum(NA)` gives `NA`, not `0` like `sum(NA, na.rm = TRUE)`, similarly for `fmin` and `fmax`).]  I also note that you can also use `dplyr::group_by` and `dplyr::select`, but `fgroup_by` and `fselect` are significantly faster (see benchmark). We can do a weighted aggregation using the variable `ODA` as weights using:

```{r}
# Weighted group mean: Weighted by ODA
wlddev %>% fgroup_by(region, income) %>% 
  fselect(PCGDP:ODA) %>% fmean(ODA) %>% head3
```

Note that in this case by default (`keep.w = TRUE`) the sum of the weights is also computed and saved. *collapse* also has its own data aggregation command called `collap`. It is in many ways more flexible, for example you can apply multiple functions to each column and efficiently reshape the result.  

```{r}
collap(wlddev, by = ~ region + income, 
       FUN = list(fmean, fsd), cols = 9:12) %>% head3
collap(wlddev, by = ~ region + income, 
       FUN = list(fmean, fsd), cols = 9:12, return = "long") %>% head3

```

`collap` also supports flexible multi-type aggregation, here applying the mean to numeric and statistical mode to categorical data. The default (`keep.col.order = TRUE`) ensures that the data remains in the same order, and, when working with *Fast Statistical Functions*, all column attributes are preserved.
```{r}
# Applying the mean to numeric and the mode to categorical data (first 2 arguments are 'by' and 'FUN')
collap(wlddev, ~ iso3c + decade, fmean, catFUN = fmode) %>% head3

# Same as a piped call.. without column reordering 
wlddev %>% fgroup_by(iso3c, decade) %>% {
  add_vars(fmode(cat_vars(.)), fmean(get_vars(., 9:12))) # cat_vars selects non-numeric (categorical) columns
} %>% head3

# Adding weights: weighted mean and weighted mode (catFUN is 3rd argument) 
collap(wlddev, ~ iso3c + decade, fmean, fmode, w = ~ ODA) %>% head3

# Fully custom aggregation (also possible with weights)
collap(wlddev, ~ iso3c + decade, 
            custom = list(fmean = 9:12, 
                          fmax = 9:10, 
                          flast = cat_vars(wlddev, "indices"),
                          fmode = "GINI")) %>% head3

```
When aggregating with multiple functions, you can parallelize over them (internally done with `parallel::mclapply`).

Time computations on panel-data are also performed fast and simple. 

```{r}
# Panel Lag and lead of PCGDP and LIFEEX
L(wlddev, -1:1, PCGDP + LIFEEX ~ iso3c, ~year) %>% head3

# Equivalent piped call
wlddev %>% fgroup_by(iso3c) %>% fselect(iso3c, year, PCGDP, LIFEEX) %>% 
  flag(-1:1, year) %>% head3

# Growth rates in percentage terms: 1 and 10-year
G(wlddev, c(1, 10), 1, ~ iso3c, ~year, cols = 9:12) %>% head3 # or use Dlog, or G(..., logdiff = TRUE) for percentages
```

Equivalently we can can compute leaded and suitably iterated (log-) differences and growth rates, as well as quasi-(log-)differences of the form $x_t - \rho x_{t-1}$. The operators `L` and `G` are shorthand's for the functions `flag` and `fgrowth` allowing formula input. Similar operators exist for `fwithin`, `fscale`, etc. 

# Benchmark

For benchmarking we use some product-level trade data from the UN Comtrade database, processed by \href{https://tradestatistics.io/}{tadestatistics.io}.

```{r, eval=FALSE}
library(tradestatistics)
# US HS4-level trade from 2000 to 2018
us_trade <- ots_create_tidy_data(years = 2000:2018, 
                                 reporters = "usa", 
                                 table = "yrpc")
```
Downloading US product-level trade (HS4) from 2000 to 2018 gives about 2.6 million observations:
```{r, echo=FALSE}
load("C:/Users/Sebastian Krantz/Documents/R/collapse/docs/blog/us_trade.RData")
settransform(us_trade, id = NULL, export_value_usd2 = NULL, import_value_usd2 = NULL)
```
```{r}
fdim(us_trade)
head3(us_trade)

# 19 years, 221 trading partners, 1222 products, unbalanced panel with product-time gaps...
qDF(rbind(class = vclasses(us_trade), 
          Ndistinct = fndistinct(us_trade)))

# Summarizing data between and within partner-product pairs
qsu(us_trade, pid = export_value_usd + import_value_usd ~ partner_iso + product_code)
```
It would also be interesting to summarize the trade flows for each partner, but that would be too large to print to the console. We can however get the `qsu` output as a list of matrices: 
```{r}
# Doing all of that by partner - variance of flows between and within traded products for each partner
l <- qsu(us_trade, by = export_value_usd + import_value_usd ~ partner_iso, 
                   pid = ~ partner_iso + product_code, array = FALSE)
str(l)
```
Now with the function `unlist2d`, we can efficiently turn this into a tidy data.table:

```{r}
unlist2d(l, idcols = c("Variable", "Trans"), 
         row.names = "Partner", DT = TRUE)
```
If `l` were some statistical object we could first pull out relevant elements using `get_elem`, possibly process those elements using `rapply2d` and then apply `unlist2d` to get the data.frame (or data.table with `DT = TRUE`). These are the main *collapse* list-processing functions. 

Now on to the benchmark. It is run on a Windows 8.1 laptop with a 2x 2.2 GHZ Intel i5 processor, 8GB DDR3 RAM and a Samsung 850 EVO SSD hard drive.

```{r, message=FALSE, warning=FALSE, error=FALSE}
library(microbenchmark)
library(dplyr)
library(data.table)
setDTthreads(2)     # Default setting for this machine

# Grouping (data.table:::forderv here does not compute the unique groups yet)
microbenchmark(collapse = fgroup_by(us_trade, partner_iso, group_code, year), 
               data.table = data.table:::forderv(us_trade, c("partner_iso", "group_code", "year"), retGrp = TRUE),
               dplyr = group_by(us_trade, partner_iso, group_code, year), times = 10)

# Sum
microbenchmark(collapse = collap(us_trade, export_value_usd + import_value_usd ~ partner_iso + group_code + year, fsum),
               data.table = us_trade[, list(export_value_usd = sum(export_value_usd, na.rm = TRUE),
                                            import_value_usd = sum(import_value_usd, na.rm = TRUE)),
                                      by = c("partner_iso", "group_code", "year")],
               dplyr = group_by(us_trade, partner_iso, group_code, year) %>%
                       dplyr::select(export_value_usd, import_value_usd) %>% summarise_all(sum, na.rm = TRUE), times = 10)

# Mean
microbenchmark(collapse = collap(us_trade, export_value_usd + import_value_usd ~ partner_iso + group_code + year, fmean),
               data.table = us_trade[, list(export_value_usd = mean(export_value_usd, na.rm = TRUE),
                                            import_value_usd = mean(import_value_usd, na.rm = TRUE)),
                                     by = c("partner_iso", "group_code", "year")],
               dplyr = group_by(us_trade, partner_iso, group_code, year) %>%
                 dplyr::select(export_value_usd, import_value_usd) %>% summarise_all(mean, na.rm = TRUE), times = 10)

# Replace with group-sum
microbenchmark(collapse = fgroup_by(us_trade, partner_iso, group_code, year) %>%
                 fselect(export_value_usd, import_value_usd) %>% fsum(TRA = "replace_fill"),
               data.table = us_trade[, `:=`(export_value_usd2 = sum(export_value_usd, na.rm = TRUE),
                                             import_value_usd2 = sum(import_value_usd, na.rm = TRUE)),
                                      by = c("partner_iso", "group_code", "year")],
               dplyr = group_by(us_trade, partner_iso, group_code, year) %>%
                 dplyr::select(export_value_usd, import_value_usd) %>% mutate_all(sum, na.rm = TRUE), times = 10)

# Centering, partner-product
microbenchmark(collapse = fgroup_by(us_trade, partner_iso, product_code) %>%
                 fselect(export_value_usd, import_value_usd) %>% fwithin,
               data.table = us_trade[, `:=`(export_value_usd2 = export_value_usd - mean(export_value_usd, na.rm = TRUE),
                                            import_value_usd2 = import_value_usd - mean(import_value_usd, na.rm = TRUE)),
                                     by = c("partner_iso", "group_code", "year")],
               dplyr = group_by(us_trade, partner_iso, group_code, year) %>%
                 dplyr::select(export_value_usd, import_value_usd) %>% mutate_all(function(x) x - mean(x, na.rm = TRUE)), times = 10)

# Lag
# Much better to sort data for dplyr
setorder(us_trade, partner_iso, product_code, year)
# We have an additional problem here: There are time-gaps within some partner-product pairs
tryCatch(L(us_trade, 1, export_value_usd + import_value_usd ~ partner_iso + product_code, ~ year), 
         error = function(e) e)
# The solution is that we create a unique id for each continuous partner-product sequence
settransform(us_trade, id = unattrib(seqid(year + unattrib(finteraction(partner_iso, product_code)) * 20L)))
# Notes: Normally id = seqid(year) would be enough on sorted data, but here we also have very different start and end dates...thus is some cases the next group starts the year after the former ends, checked with: table(with(us_trade, ffirst(fdiff(year), list(partner_iso, product_code))) == 1). unattrib removes all attributes, giving a plain integer id (just done for the benchmark here, otherwise collapse would have an advantage).

fndistinct(us_trade$id) 
# Another nice comparison... fmode is similarly fast for categorical aggregation...
microbenchmark(fndistinct(us_trade$id), n_distinct(us_trade$id))

# Here we go now: 
microbenchmark(collapse = L(us_trade, 1, export_value_usd + import_value_usd ~ id),
               collapse_ordered = L(us_trade, 1, export_value_usd + import_value_usd ~ id, ~ year),
               data.table = us_trade[, shift(.SD), keyby = id,
                                     .SDcols = c("export_value_usd","import_value_usd")],
               data.table_ordered = us_trade[order(year), shift(.SD), keyby = id,
                                     .SDcols = c("export_value_usd","import_value_usd")],
               dplyr = group_by(us_trade, id) %>% dplyr::select(export_value_usd, import_value_usd) %>%
                       mutate_all(lag), times = 10)

# Note: you can do ordered lags using mutate_all(lag, order_by = "year") for dplyr, but at computation times in excess of 90 seconds..

```

The benchmark shows that *collapse* is consistently very fast for the operations it provides (this also pertains to weighted computations which are only slightly slower than unweighted ones). But of course *collapse* cannot do a lot of things you can do with *dplyr* or *data.table* and vice-versa. It is and remains an advanced package, but I think it lives up to the high standards set forth by these packages. I am also highly indebted to *data.table* for inspiration and some vital bits of C-code. Feel free to get in touch for any suggestions or comments about *collapse*. I hope you will find it useful. 

```{r, echo=FALSE}
options(oldopts)
```
